# VIETNAM TKNZ

* Rule #1: SYLLABLES are FIRST CLASS Citizens
* Rule #2: SubWord must re-use known SYLLABLES
* Rule #3: Ph·∫£i t√°ch ƒë∆∞·ª£c tokens b·ªã d√≠nh
	=> Ch·∫•p nh·∫≠n nhi·ªÅu ·ª©ng c·ª≠ vi√™n ti·ªÅm nƒÉng, ch·ªçn l·ªçc sau!
* Rule #4: Don't try to be clever
* Rule #5: Prefer simple solution!


## Phase-1: Space-splitter (space-32, tab-9) and alphabet vs nonalpha splitter

TODO:

* Handle `Tho·ªçng`: need to conver `oo` to `ooo` before passing to syll-parser

* Reject mixed upper vs lower case syllable, keep only titelized or capitalized sylls

* Break alphabet-types into have-marks-and-tone vs others
FIX: why "cp" is categoried as marktone?

[ DONE ]

* colapse types that have similar _ascii-telex trans_ to create next layer of _ascii-telex types_

* Convert _utf8 types_ to _ascii-telex types_

* Find an efficient way to check if a _utf8 type_ is a Vi Syllable or not!
  - We know that `am_giua` is a must for a Vi Syllable
  - Some `am_giua` can follow one some `am_dau` and only be followed by some `am_cuoi`

* Choose an efficient bytes-hashing algorithm to count similar _utf8 lower_case-fom tokens_ to create _utf8 types_ (see `_HASH.md`)

### One-Way-Mapping Multiple layer of (Re-)presentations:
1. utf8 bytes => 2.
2. utf8 non-breaking tokens => 3.
3. utf8 alphabet tokens & utf8 delimiter (non-alphabet) tokens => 4.
4. utf8 alphabet types => 5. & utf8 delimiter (non-alphabet) types
5. ascii-telex syllable types (syllable vocab)

The purpose of this phase is to split the input text into a list of non-breaking tokens separated by space characters that can be inputted from keyboard (32, 9).

It's could be Syllables `Ch√¢y   √¨   n·ªôp   ph·∫°t   ngu·ªôi   Ch√°u   ƒë√≤i   ti·ªÅn   c∆°m   d√¨   nh√†   ƒê√†   N·∫µng   nghi√™n   c·ª©u   ti·ªán   √≠ch   nh·∫Øn   tin   khi   vi   ph·∫°m   ƒë·∫øn   ch·ªß   ph∆∞∆°ng   Kh√≥   x·ª≠   v·ª•   m·∫π   tu·ªïi   tr·ªôm   xe   h∆°i   c·ªßa   con   g√°i   Thay   ƒë·ªïi   v·ªÅ   ƒëƒÉng   k√Ω   chuy·ªÉn   nh∆∞·ª£ng   t·ª´   b·∫°n   c·∫ßn   bi·∫øt   Nh·ªØng   tr∆∞·ªùng   h·ª£p   tr∆∞ng   c·∫ßu   gi√°m   ƒë·ªãnh   trong   √°n   kinh   t·∫ø   Th·ªã   tr·∫•n   ·ªü   b√°n   v·ªõi   gi√°   h∆°n   ƒë·ªÉ   thu   h√∫t   c∆∞   d√¢n   B·ªè   quy`

Or non-alphabet / abbr / foreign-words ... `.   ,   70   12/2   1   12/2/2018:   20   :   '   '.   12/2.   20.000   02/2018.   2/2018?.   2/2018.   12/2:   24.000   2   ?.   -   12/02/2018,   18   (   ).   12   !'.   12:   7/18.   12/2/2018.   m2   BOT   18   QL18   TPHCM   CAND   7   FLC   4   SEA   Games   PVP   Land   U23   6km   MC   68   3   Samsung   Display   300   VFF   29   8   TNCN   AFF   Cup   2008   23   Italy   euro   200   Vietlott   105   27   21   casino   1986   FDI   jeans   DNNVV   bikini   TP   HCM   25   30   Rolls   Royce   Bespoke   2017   Cagliari   Juventus   HLV   Allegri   Serie   Icardi   Inter   80   4000   26   Rome   Mourinho   Morata   C1   Real   Ronaldo   VN   K   BHXH   THPT   Myanmar   Rohingya   TAND   T   ara   Facebook   Clip   Mercedes   container   Venezuela   265   Google   Uber   Aerobic   260   16   Malaysia   Chol`,

Or look-like-Vietnamese (typo, abbr, borrowed-words ...) `BHYTN√¢ng   ƒêHTB   B√¥lykhƒÉmxay   n√†y15   X√¥vi·∫øt   i·ªët   N·ªôiTh√≠ch   HƒêBA   cr√¥m   Chil√™   HƒêLƒê   uy√™nhX√≠   Hr√™   Kr√¥ng   BƒêKH   ƒë√¥la   ƒêHQG   Eur√©ka   QSƒê   ƒë√≥nT·∫øt   Ti·∫øg   to√†ndi·ªán   z√°o   z·ª•k   ƒêT601   LƒêLƒêVN   LƒêTB   zƒÉng   CQƒêT   ƒë√¥l√¥mit   Tho·ªçng   V·∫Øcxin   ƒêVHD   √°Bo   PTƒê   CƒêCS   xt√™   GƒêKT   k√™t   s∆°mi   QƒêND   ATVSLƒê   M√¥t   h·∫°iB√†i`


This phase must:

* Moving fast and try to detect as much syllable-tokens using strict rules [$] as possible!

* Convert utf-8 syllable-token to ascii-telex syllable-token

* Treat newline-10 as a special token type

* Adding class-attribute to each token 1 => Syllable, 2 => Newline 3 => Others

* Counting similar tokens to create types

[$] Strick rules ensure that only utf-8 token that 100% look like a vi-syllable with 0-confusion is converted. VD: `Ng∆∞∆°√¨ => Nguwowif` but not confusing case like `ng∆∞·ªù√≠` or `c√°iiii g√¨????` ...

```js
Input: "Ng∆∞∆°√¨ ∆°iii ch√†o nh√©!"
Output: "Nguowif", "∆°iii", "chaof", "nhes", "!"
```

By doing all of this we respect Rule #1/ that makes "SYLLABLES are FIRST CLASS Citizens" so we can build a syllable vocab, and prepare input data the next phase.


## Phase-2: Subword segmentation

https://everdark.github.io/k9/notebooks/ml/natural_language_understanding/subword_units/subword_units.nb.html#12_probablistic_subword_segmentation

We iterate over every position in a given word. At each end-of-character position, we determine the best segment by finding the one with highest likelihood given the current vocabulary.

we need to have a vocabulary for subwords that can attribute each subword to a probability. We can use BPE to build such vocabulary. For complete coverage we will also include character-level subwords into the vocabulary.

https://everdark.github.io/k9/notebooks/ml/natural_language_understanding/subword_units/subword_units.nb.html#123_em_with_viterbi

Now we know the idea of EM, and we know how to find the optimal segment path by Viterbi, we can put them together to forumlate the optimization task of our probablistic subword segmentation.

* Initialize a large seeding subword vocabulary from the training corpus
* [Expectation] Estimate each subword probability by the corresponding frequency counts in the vocabulary
* [Maximization] Use Viterbi to segment the corpus, returning the optimal segments
* Compute the loss of each new subword from optimal segments
* Shrink the vocabulary size by dropping the subwords with top X% smallest losses
* Repeat step 2 to 5 until the vocabulary size reaches a desired number

The loss of a subword in step 4 is the reduction in overall training corpus segment likelihood if that subword is removed from the current vocabulary.

we used BPE to generate the initial vocabulary. Another common choice is to the suffix array algorithm. to generate the common substrings.


```js
Input: "Nguowif", "∆°iii", "chaof", "nhes", "!"
Output: "Nguowif", "owi", "i", "i", "chaof", "nhes", "!"
```
After phase-#1, the number of others-tokens is quite small. For above example we need to process token number 3 only!


TODO:

* S·ª≠ d·ª•ng tokens d·∫°ng nguy√™n b·∫£n utf8 c·ªßa vocab ƒë·ªÉ scan OOV types, nh·∫±m t√°ch c√°c th√†nh ph·∫ßn ch·∫Øc ch·∫Øn l√† √¢m ti·∫øt. Ch√∫ √Ω c√°ch ph√¢n bi·ªát ch·ªØ c√°i vi·∫øt hoa vs ch·ªØ c√°i vi·∫øt th∆∞·ªùng c√≥ th·ªÉ d√πng ƒë·ªÉ l√†m boundary ƒë·ªÉ t√°ch (xem v√≠ d·ª• d∆∞·ªõi).

* C√≥ nhi·ªÅu c√°ch t√°ch th√¨ gi·ªØ l·∫°i th√†nh nhi·ªÅu d·ªã b·∫£n, c√†ng nhi·ªÅu ·ª©ng c·ª≠ vi√™n ti·ªÅm nƒÉng c√†ng t·ªët, c√¢n h·∫øt!

* Choose an effective subword tknz algo that suitale for Vi and reuse syllable vocab

```js
Try: "M·∫πH√†My" -> "M·∫πH√†M|y" -> true
Try: "M·∫πnu√¥i" -> "M·∫πn|u√¥i" -> true
Try: "NgheNh|√¨n" -> false
Try: "M√¥n|√¥l√¥x·ªëp" -> "M√¥|n√¥|l√¥|x·ªëp" ?? n√™n gi·ªØ nguy√™n v√¨ ƒë√¢y l√† t√™n ri√™ng
```

This phase must:

* Re-use syllable vocab to break tokens in meaningful parts

* Combining subword tokenize techniques to obtain best results


## Phase-3: Anything elses / Post-processing

Digit tknz?, Vi·∫øt t·∫Øt? ...

- - - 

# REF: SoTA HF Tokenizer
https://huggingface.co/docs/tokenizers/python/latest/components.html

## Normalizer
* Unicode normalization algorithms (NFD, NFKD, NFC & NFKC)
* Lowercasing
* Strip: Removes all whitespace characters on the specified sides (left, right or both) of the input
* ...
* Sequence: Composes multiple normalizers that will run in the provided order 
	`Sequence([NFKC(), Lowercase()])`

## PreTokenizer

The PreTokenizer takes care of splitting the input according to a set of rules. This pre-processing lets you ensure that the underlying Model does not build tokens across multiple ‚Äúsplits‚Äù. For example if you don‚Äôt want to have whitespaces inside a token, then you can have a PreTokenizer that splits on these whitespaces.

### ByteLevel Spliting (will be used with BPE)

Splits on whitespaces while remapping all the bytes to a set of visible characters. This technique as been introduced by OpenAI with GPT-2 and has some more or less nice properties:

	* Since it maps on bytes, a tokenizer using this only requires 256 characters as initial alphabet (the number of values a byte can have), as opposed to the 130,000+ Unicode characters.

	* A consequence of the previous point is that it is absolutely unnecessary to have an unknown token using this since we can represent anything with 256 tokens (Youhou!! üéâüéâ)

	* For non ascii characters, it gets completely unreadable, but it works nonetheless!


## BPE

One of the most popular subword tokenization algorithm. The Byte-Pair-Encoding works by starting with characters, while merging those that are the most frequently seen together, thus creating new tokens. It then works iteratively to build new tokens out of the most frequent pairs it sees in a corpus.

BPE is able to build words it has never seen by using multiple subword tokens, and thus requires smaller vocabularies, with less chances of having ‚Äúunk‚Äù (unknown) tokens.


## WordPiece

This is a subword tokenization algorithm quite similar to BPE, used mainly by Google in models like BERT. It uses a greedy algorithm, that tries to build long words first, splitting in multiple tokens when entire words don‚Äôt exist in the vocabulary. This is different from BPE that starts from characters, building bigger tokens as possible.

It uses the famous ## prefix to identify tokens that are part of a word (ie not starting a word).

## Unigram

Unigram is also a subword tokenization algorithm, and works by trying to identify the best set of subword tokens to maximize the probability for a given sentence. This is different from BPE in the way that this is not deterministic based on a set of rules applied sequentially. Instead Unigram will be able to compute multiple ways of tokenizing, while choosing the most probable one.


- - - 

## Blah, blah, blah ...

https://blog.floydhub.com/tokenization-nlp/

TOKEN, TOKENIZE, TOKENIZATION: WHAT'S TOKEN? WHY TOKENIZATION? TOKENIZE FOR WHAT?

@S√°ng 15/06/2021 trong l√∫c chu·∫©n ho√° c√°c vi·ªác s·ª≠ d·ª•ng `terms` (see notes_on_terms.md), ch·ª£t nh·∫≠n ra m√¨nh ƒëang thi·∫øu s·ª± hi·ªÉu ƒë√∫ng v·ªÅ th·∫ø n√†o `token`, t√¨m ki·∫øm tr√™n Google v√† t√¨m ƒë·∫øn [0]. B√†i vi·∫øt ch√¢n ph∆∞∆°ng, d·ªÖ hi·ªÉu khi·∫øn m√¨nh nh·∫≠n ra:

Token hi·ªÉu ƒë∆°n gi·∫£n l√† ƒë·∫ßu ra c·ªßa qu√° tr√¨nh bi·∫øn ƒë·ªïi c√°c c√¢u ch·ªØ ƒë·∫ßu v√†o (text) th√†nh c√°c ƒë∆°n-v·ªã n·ªÅn t·∫£ng ƒë·ªÉ l√†m c∆°-s·ªü cho c√°c b∆∞·ªõc x·ª≠ l√Ω ti·∫øp theo.

Nh∆∞ v·∫≠y ƒë·ªãnh nghƒ©a v·ªÅ `token`, `tokenize` ... l√† r·ªông, kh√¥ng c·ªë k·∫øt v·ªõi b·∫•t c·ª© th·ª© g√¨ c·ª• th·ªÉ. C√°ch hi·ªÉu tokenization l√† t√°ch-t·ª´ nh∆∞ m√¨nh v·∫´n c√≥ trong ƒë·∫ßu l√† b·ªã thi√™n l·ªách, ch∆∞a th·∫•u ƒë·∫°t v√† tho·∫£ ƒë√°ng.

B√†i vi·∫øt g·ª£i ra 1 t·ª´ kho√° l√†m m√¨nh t√≤ m√≤ `Subword tokens` (Character tokens: s-m-a-r-t-e-r vs Subword tokens: smart-er) v√¨ t·ª´ tr∆∞·ªõc t·ªõi nay m√¨nh ch∆∞a ƒë√†o s√¢u m√† ch·ªâ hi·ªÉu n√¥m na tokenization l√† t√°ch-t·ª´, m√† gi·ªù c√≤n t√°ch-nh·ªè-h∆°n-c·∫£-t·ª´ n·ªØa c∆° √†? 

Th·ª±c ra t√°ch nh·ªè h∆°n t·ª´ l√† chuy·ªán th∆∞·ªùng nh∆∞ c√¢n ƒë∆∞·ªùng h·ªôp s·ªØa, ti·∫øng Anh c√≥ prefix, suffix, root ..., ti·∫øng Vi·ªát c√≥ c√°ch ph√¢n t√°ch √¢m ti·∫øt nh∆∞ sau (th√¨ ch√≠nh l√† subword r·ªìi c√≤n g√¨ :^)
```js
√¢m ti·∫øt = ph·ª• √¢m ƒë·∫ßu + v·∫ßn
v·∫ßn = √¢m gi·ªØa + √¢m cu·ªëi + thanh ƒëi·ªáu
√¢m gi·ªØa = √¢m ƒë·ªám + nguy√™n √¢m ch√≠nh

Note: nguy√™n √¢m ch√≠nh lu√¥n c√≥ v√† thanh ƒëi·ªáu lu√¥n c√≥
Ph·ª• √¢m ƒë·∫ßu, √¢m ƒë·ªám, v√† √¢m cu·ªëi c√≥ th·ªÉ ko c√≥. 
V·∫ßn lu√¥n c√≥ v√† thanh ƒëi·ªáu l√† 1 thu·ªôc t√≠nh c·ªßa v·∫ßn, bao tr√πm to√†n b·ªô v·∫ßn

VD1: c√°ch ph√¢n t√°ch theo ph√°t √¢m

g·∫∑t		g {a2 t}_T6
ch∆∞a	ch ua2_T1
ƒë∆∞·ª£c	d2 {ua2 k}_T6
h√¨nh	h {i nh}_T2
th√†nh	th {a1 nh}_T2
th√≥i	th oi_T3
quen	k {u1 e1 n}_T1

VD1: c√°ch ph√¢n t√°ch theo nh·∫≠p li·ªáu TELEX t·ª´ b√†n ph√≠m
Quy ∆∞·ªõc b·ªè d·∫•u ngay sau nguy√™n √¢m v√† thanh ƒëi·ªáu g√µ cu·ªëi c√πng

g·∫∑t		g aw t j
ch∆∞a	ch uwa
ƒë∆∞·ª£c	dd uwow c j
h√¨nh	h i nh f
th√†nh	th a nh f
th√≥i	th oi s
quen	qu e n

```

M√¨nh v·ª´a l√†m xong module encode √¢m ti·∫øt ti·∫øng Vi·ªát th√†nh `u17` (d√πng 17-bits) trong ƒë√≥ {ph·ª• √¢m ƒë·∫ßu} c·∫ßn `u5`, {nguy√™n √¢m ƒë·ªám + nguy√™n √¢m ch√≠nh} c·∫ßn `u5`, {√¢m cu·ªëi} c·∫ßn `u4` v√† {thanh ƒëi·ªáu} c·∫ßn `u3` n√™n √Ω th·ª©c r√µ v·ªÅ subwords trong ti·∫øng Vi·ªát, c·ªông th√™m c√°c ki·ªÉu bi·∫øn th·ªÉ vi·∫øt t·∫Øt, ti·∫øng v√πng mi·ªÅn, vi·∫øt ki·ªÉu "tr·∫ª tr√¢u" tr√™n internet n·ªØa th√¨ subwords "z√¥ c√πngg ƒëa ja.ng v√† th√∫ v·ªã·ªã·ªã·ªã·ªã·ªã·ªã". 

N·∫øu ch·ªâ th√¥ng qua chu·∫©n ho√° ƒë·∫ßu ƒë·ªÉ quy chu·∫©n c√°c bi·∫øn th·ªÉ ƒë√≥ th√†nh √¢m ti·∫øt g·ªëc th√¨ ƒë·ªùi c√≤n g√¨ l√† "v√¥ c√πng ƒëa d·∫°ng v√† th√∫ v·ªã" n·ªØa :'( B·ªüi ch√∫ng ta ƒë√£ l√†m m·∫•t ƒëi nhi·ªÅu th√¥ng tin kh√°c m√† ng∆∞·ªùi g√µ mu·ªën chuy·ªÉn t·∫£i qua con ch·ªØ nh∆∞ l√† t√¢m tr·∫°ng, v√πng mi·ªÅn, hay ƒë∆°n gi·∫£n l√† "g√µ k√©m", "l√≥ng ng√≥ng", "m·∫Øt m·ªù, tay ch·∫≠m", "ch√≠nh t·∫£ d·ªët" n√™n g√µ nh·∫ßm ho√†i!

Ti·∫øp ƒë√≥, b√†i vi·∫øt th·ªü ra m·ªôt c√¢u ch·ªët "Transformed based models ‚Äì the SOTA in NLP ‚Äì rely on Subword Tokenization algorithms for preparing vocabulary. One of the most popular Subword Tokenization algorithm known as Byte Pair Encoding (BPE) that tackles OOV effectively."

T·∫°m d·ªãch: C√°c M√¥-h√¨nh "bi·∫øn ƒë·ªïi √°nh x·∫°", (m√¥ h√¨nh) X·ª≠-L√Ω-Ng√¥n-Ng·ªØ-T·ª±-Nhi√™n ƒë∆∞∆°ng ƒë·∫°i, d·ª±a v√†o c√°c thu·∫≠t to√°n `Subword Tokenization` ƒë·ªÉ chu·∫©n b·ªã t·ª´ v·ª±ng. M√£ ho√° c·∫∑p-byte (BPE) l√† thu·∫≠t to√°n `Subword Tokenization` "qu·ªëc d√¢n" nh·∫•t, n√≥ gi·∫£i quy·∫øt OOV hi·ªáu qu·∫£!

VD: word-embedding bi·∫øn-ƒë·ªïi tokens b·∫±ng c√°ch √°nh x·∫° n√≥ 1-1 v·ªõi c√°c vectors to√°n h·ªçc c·ª±c nhi·ªÅu chi·ªÅu (v√†i trƒÉm ch·∫≥ng h·∫°n) [1]. 

M√¨nh v·ª´a `tokenize` 1 copus g·∫ßn 600MB text ti·∫øng Vi·ªát v√† OOV kh√° nhi·ªÅu n√™n b·ªì k·∫øt ngay qu·∫£ "gi·∫£i quy·∫øt OOV hi·ªáu qu·∫£". N√≥i v·∫≠y ƒë·ªß ƒë·ªÉ hi·ªÉu subword tokenization r·∫•t hay ho :^)

- - -

Lu·∫≠n v·ªÅ x·ª≠ l√Ω th√¥ng tin, con ng∆∞·ªùi hay m√°y t√≠nh kh√¥ng ph·∫£i l√∫c n√†o c≈©ng nh·∫≠n ƒë∆∞·ª£c t√≠n hi·ªáu ƒë·∫ßu v√†o CHU·∫®N m√† hai b√™n ƒë√£ quy ∆∞·ªõc v·ªõi nhau t·ª´ tr∆∞·ªõc. Ch√∫ng ta ph·∫£i suy ƒëo√°n (a.k.a gi·∫£i m√£) r·∫•t nhi·ªÅu ƒë·ªÉ c√≥ th·ªÉ hi·ªÉu ƒë∆∞·ª£c ng√¥n ng·ªØ, trong ƒë·ªëi tho·∫°i h√†ng ng√†y ch·∫≥ng h·∫°n: 
+ c√≥ ng∆∞·ªùi n√≥i r·∫•t nhanh, n√≥i t·∫Øt, n√≥i thi·∫øu t·ª´ (m·∫•t t√≠n hi·ªáu -  signal lost/uncompleted)
+ c√≥ ng∆∞·ªùi n√≥i r·ªÅ r√†, ·ªÅ √†, lo·∫±ng ngo·∫±ng (nhi·ªÖu t√≠n hi·ªáu - signal vs noise)
+ c√≥ ng∆∞·ªùi d√πng ti·∫øng ƒë·ªãa ph∆∞∆°ng, b·ªìi th√™m ti·∫øng n∆∞·ªõc ngo√†i (nh·∫≠n ƒë∆∞·ª£c t√≠n hi·ªáu nh∆∞ng thi·∫øu th√¥ng tin v·ªÅ t√≠n hi·ªáu ƒë√≥ - unknown signal)

Trong x·ª≠ l√Ω t√≠n hi·ªáu (c√≥ li√™n quan t·ªõi m√°y t√≠nh) ƒë√≥ l√† qu√° tr√¨nh kh·ª≠ nhi·ªÖu, kh√¥i ph·ª•c t√≠n hi·ªáu b·ªã m·∫•t t·ª´ nh·ªØng t√≠n hi·ªáu ƒë√£ c√≥, gi·∫£i m√£ t√≠n hi·ªáu b·∫±ng nhi·ªÅu m√¥ h√¨nh kh√°c nhau ...

Khi g√µ/chat c≈©ng th·∫ø, g√µ sai ch√≠nh t·∫£; g√µ t·∫Øt cho nhanh; g√µ thi·∫øu d·∫•u, thi·∫øu thanh ƒëi·ªáu; n√£o ch√∫ng ta ph·∫£i suy ƒëo√°n (gi·∫£i m√£) r·∫•t l√† nhi·ªÅu, c√≤n ch∆∞a k·ªÉ y·∫øu t·ªë t·ª´ ƒë·ªìng √¢m kh√°c nghƒ©a, t·ª´ ƒëa nghƒ©a ... n·ªØa, n√≥ bu·ªôc ch√∫ng ta ph·∫£i H·ªåC li√™n t·ª•c: H·ªçc b·∫±ng c√°ch ph·∫£i N·∫†P th√¥ng tin chung quanh ch·ªß ƒë·ªÅ ƒëang trao ƒë·ªïi, b·∫±ng c√°ch di·ªÖn gi·∫£i l·∫°i v√† h·ªèi ƒë·ªëi ph∆∞∆°ng xem m√¨nh hi·ªÉu nh∆∞ v·∫≠y c√≥ ƒë√∫ng kh√¥ng, b·∫±ng va ch·∫°m th·ª≠ sai (trials and errors) ƒë·ªÉ t·ª± r√∫t kinh nghi·ªám. Khi h·ªçc ƒë·ªß r·ªìi th√¨ ch·ªâ c·∫ßn ng∆∞·ªùi kia bu·ªôt mi·ªáng 1 c√°i l√† m√¨nh bi·∫øt h·ªç ƒë·ªãnh n√≥i c√°i g√¨ r·ªìi, ho·∫∑c cho c·∫≠u g√µ t·∫Øt tho·∫£i m√°i t·ªõ li·∫øc c√°i l√† hi·ªÉu li·ªÅn, ho·∫∑c nh∆∞ trong khi chat v·ªõi nhau th·∫•y b√™n kia c·ª© ng·∫≠p ng·ª´ng, g√µ r·ªìi m√† ch∆∞a th·∫•y g·ª≠i ƒëi l√† ƒëo√°n h·ªç ƒëang c√≥ ƒëi·ªÅu g√¨ ng·∫≠p ng·ª´ng kh√≥ n√≥i ƒë√¢y ... 

T·ª©c l√† ng√¥n ng·ªØ ch·ªâ l√† ph∆∞∆°ng ti·ªán ƒë·ªÉ ng∆∞·ªùi ta truy·ªÅn t·∫£i truy·ªÅn t·∫£i √Ω ƒë·ªãnh / √Ω mu·ªën c·ªßa m√¨nh cho ng∆∞·ªùi kh√°c hi·ªÉu ƒë∆∞·ª£c, n√≥ ch·ªâ l√† ph∆∞∆°ng ti·ªán, l√† k√™nh truy·ªÅn t·∫£i kh√¥ng h∆°n. Trong k√™nh truy·ªÅn t·∫£i ƒë√≥ ngo√†i t·ª´ ng·ªØ, ng√¥n ng·ªØ c√≤n nhi·ªÅu th√¥ng tin n·ªØa v·ªÅ ho√†n c·∫£nh, tr·∫°ng th√°i c·∫£m x√∫c, s·ª©c kho·∫ª, vƒÉn ho√°, v√πng mi·ªÅn, th·ªùi ti·∫øt (l·∫°nh qu√° n√≥i run l·∫≠p c·∫≠p), b√†n ph√≠m t9 (ƒëi·ªán tho·∫°i c·ªï ch·ªâ c√≥ 10 ph√≠m 0-9 ƒë∆∞·ª£c l·ªìng gh√©p ch·ªØ c√°i a-z v√†o ƒë√≥) kh√¥ng th·ªÉ g√µ ƒë·ªß d·∫•u ƒë∆∞·ª£c ... ƒë∆∞·ª£c truy·ªÅn t·∫£i theo m·ªôt c√°ch tr·ª±c ti·∫øp ho·∫∑c gi√°n ti·∫øp. Nh·ªØng th√¥ng tin ƒë√≥ gi√∫p ch√∫ng ta (v√† m√°y t√≠nh) hi·ªÉu s√¢u h∆°n v·ªÅ th√¥ng ƒëi·ªáp mu·ªën ƒë∆∞·ª£c truy·ªÅn t·∫£i, th√¥ng c·∫£m h∆°n v·ªõi ng∆∞·ªùi n√≥i / g√µ, d·ª±a v√†o ƒë√≥ m√† ph·∫£n h·ªìi k·ªãp th·ªùi, ƒë√∫ng tr·ªçng t√¢m, tho·∫£ m√£n hai b√™n, khi·∫øn c·∫£ hai ƒë·∫°t ƒë∆∞·ª£c tr·∫°ng th√°i m√£n nguy·ªán. Theo m√¨nh nh∆∞ v·∫≠y m·ªõi g·ªçi l√† giao ti·∫øp c√≥ t√¢m. Nh∆∞ v·∫≠y m·ªõi l√† m·ª•c ƒë√≠ch c·∫ßn ƒë·∫°t ƒë∆∞·ª£c c·ªßa x·ª≠ l√Ω th√¥ng tin, x·ª≠ l√Ω ng√¥n ng·ªØ ...

Vi·ªác chu·∫©n ho√° t∆∞·ªüng l√† hay nh∆∞ng l·∫°i l√†m m·∫•t ƒëi c√°c t√≠n hi·ªáu m√† ng∆∞·ªùi chu·∫©n ho√° cho r·∫±ng l√† kh√¥ng c·∫ßn thi·∫øt, n√≥ l√†m cho vi·ªác gi·∫£i m√£ tr·ªü n√™n ƒë∆°n gi·∫£n h∆°n v·ªõi ng∆∞·ªùi ho·∫∑c m√°y t√≠nh nh∆∞ng l√†m xa r·ªùi ƒëi m·ª•c ƒë√≠ch c·∫ßn ƒë·∫°t ƒë∆∞·ª£c ·ªü tr√™n. Khi g√µ vƒÉn b·∫£n c≈©ng th·∫ø, t√≠n hi·ªáu nguy√™n b·∫£n nh·∫•t m√† m√°y t√≠nh nh·∫≠n ƒë∆∞·ª£c ch√≠nh l√† c√°c t√≠n hi·ªáu nh·∫≠n ƒë∆∞·ª£c t·ª´ b√†n ph√≠m (key-strokes), l√†m sao ƒë·ªÉ gi·ªØ ƒë∆∞·ª£c nguy√™n b·∫£n c√°c key-strokes ƒë√≥, x·ª≠ l√Ω tr·ª±c ti·∫øp tr√™n c√°c key-strokes ƒë√≥ s·∫Ω l√†m cho b√†i to√°n tr·ªü n√™n sinh ƒë·ªông v√† h·ªØu √≠ch h∆°n r·∫•t nhi·ªÅu so v·ªõi vi·ªác ch·ªâ coi vƒÉn b·∫£n l√† 1 chu·ªói c√°c k√Ω t·ª±a ƒë∆∞·ª£c m√£ ho√° (thu·ªü x∆∞a l√† ascii, th·ªßa nay l√† unicode/utf-8).

EDGE CASES MATTER!

V·ªõi g√≥c nh√¨n tr√™n ngo√†i key-strokes (ch·ªâ nghi nh·∫≠n ƒë∆∞·ª£c th√¥ng qua b·ªô g√µ ti·∫øng Vi·ªát) ch√≠nh c√°c OOV (out-of-vocabulary words) m·ªõi l√† th·ª© sinh ƒë·ªông, th√∫ v·ªã ƒë√°ng ƒë·ªÉ ƒë√†o sau t√¨m hi·ªÉu, xem th√¥ng ƒëi·ªáp ƒë·∫±ng sau ƒë√≥ l√† g√¨ ... ?

Ti·∫øng Vi·ªát l·ªèng l·∫ªo n√™n kh·∫£ nƒÉng CH∆†I ch·ªØ r·∫•t nhi·ªÅu, n√≥i c√°ch kh√°c l√† c√°c bi·∫øn th·ªÉ v√† th√¥ng tin th√™m ƒëi k√®m c√°c bi·∫øn th·ªÉ ƒë√≥ r·∫•t nhi·ªÅu ...

√îNG CH·ª¶ V√Ä C√îNG D√ÇN H·∫†NG NH·∫§T, H·∫†NG 2,3

B·ªô g√µ ti·∫øng Vi·ªát, l√† m·ªôt b√†i to√°n c·ª±c hay m√† ch∆∞a ƒë∆∞·ª£c khai th√°c h·∫øt, v√¨ ch·ªâ c√≥ b·ªô g√µ m·ªõi nh·∫≠n ƒë∆∞·ª£c th√¥ng tin t·ª´ b√†n ph√≠m qua th·ªùi gian th·ª±c m·ªôt c√°ch nguy√™n b·∫£n v√† tr·∫ßn tr·ª•i nh·∫•t. N√≥ gi·ªëng nh∆∞ ch√∫ng ta nghe/h·ªçc tr·ª±c ti·∫øp v√† c√≥ th·ªÉ t∆∞∆°ng t√°c h·ªèi ƒë√°p l·∫°i ng∆∞·ªùi ƒëang n√≥i/gi·∫£ng v·ªõi vi·ªác ch√∫ng ta nghe l·∫°i / xem l·∫°i tr√™n youtube: Ho√†n to√†n kh√°c bi·ªát, v√† ·ªü m·ªôt ƒë·∫≥ng c·∫•p kh√°c, ko th·ªÉ so s√°nh v·ªõi nhau.

Nh∆∞ v·∫≠y b·ªô g√µ ch√≠nh l√† c√¥ng d√¢n h·∫°ng nh·∫•t, OOV l√† c√¥ng d√¢n h·∫°ng hai, b·ªô t·ª´ v·ª±ng (ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a t·ª´ tr∆∞·ªõc trong t·ª´ ƒëi·ªÉn) l√† c√¥ng d√¢n h·∫°ng ba. C√¥ng d√¢n h·∫°ng nh·∫•t theo t√∫ng c√¥ng d√¢n h·∫°ng hai v√† ba. Ng∆∞·ªùi g√µ ph√≠m l√† √¥ng ch·ªß, l√† tr√πm cu·ªëi. M√¨nh th√¥ng qua c√¥ng d√¢n ƒë·ªÉ hi·ªÉu v√† t∆∞∆°ng t√°c v·ªõi tr√πm cu·ªëi. ƒê∆∞·ª£c quy·ªÅn v√† ƒë∆∞·ª£c l·ª±a ch·ªçn t∆∞∆°ng t√°c v·ªõi c√¥ng d√¢n h·∫°ng n√†o l√† m·ªôt l·ª£i th·∫ø, l·ªõn trong c√°ch ti·∫øp c·∫≠n r·ªìi. Sau ƒë√≥ m·ªõi l√† c√°c ph∆∞∆°ng ph√°p (google v√¥ s·ªë) ƒë·ªÉ x·ª≠ l√Ω b√†i to√°n.

K·∫æT

C√°ch ti·∫øp c·∫≠n `subword` s·∫Ω l√† n·ªÅn t·∫£ng, c√°ch ti·∫øp c·∫≠n `keystrokes` s·∫Ω l√† b∆∞·ªõc ngo·∫∑t trong nh·∫≠p li·ªáu v√† t∆∞∆°ng t√°c ng∆∞·ªùi m√°y th√¥ng qua ng√¥n ng·ªØ t·ª± nhi√™n! V√† TOKEN, TOKENIZE, TOKENIZATION: WHAT IS TOKEN?, WHY TOKENIZATION?, TOKENIZE FOR WHAT? l√† b∆∞·ªõc kh·ªüi ƒë·∫ßu v√¥ c√πng quan tr·ªçng cho vi·ªác ph√°t tri·ªÉn v·ªÅ sau. B·ªüi ƒë√≥ l√† qu√° tr√¨nh ƒë·ªãnh nghƒ©a v√† t·∫°o ra c√°c ƒë∆°n v·ªã th√¥ng tin l√†m n·ªÅn t·∫£ng cho c√°c b∆∞·ªõc x·ª≠ l√Ω ti·∫øp theo. 

C√°c ƒë∆°n v·ªã c·∫ßn mang th√¥ng tin c√≥ √Ω nghƒ©a, s·ªë l∆∞·ª£ng v·ª´a ƒë·ªß (qu√° √≠t th√¨ kh√¥ c·ª©ng, qu√° nhi·ªÅu th√¨ lo·∫°n), c√≥ kh·∫£ nƒÉng ch·ªãu l·ªói, c√≥ kh·∫£ nƒÉng m·ªü r·ªông ƒë∆∞·ª£c, c√≥ kh·∫£ nƒÉng chuy·ªÉn ho√° ƒë∆∞·ª£c th√†nh chu·ªói t√≠n hi·ªáu ƒë·∫ßu v√†o g·ªëc (chuy·ªÉn ho√° hai chi·ªÅu), kh√¥ng b·ªã gi·ªõi h·∫°n b·ªüi quy ∆∞·ªõc (c√°ch g√µ, c√°ch m√£ ho√°), b·ªüi ng√¥n ng·ªØ ... s·∫Ω l√† ti√™u chu·∫©n ƒë·ªÉ ph√°t tri·ªÉn v√† x√¢y d·ª±ng TOKENIZERS. N√™n nh·ªõ word-tokenizer kh√°c, syllable-tokenizer kh√°c, subword-tokenizer kh√°c v√† keystroke-tokenizer l√† 1 th·ª© kh√°c n·ªØa ...

- - -

PH·ª§ L·ª§C: D√¢y m∆° r·ªÖ m√° v√† ƒë·ªçc th√™m

Google v·ªÅ BPE d·∫´n t·ªõi [2], [2] d·∫´n t·ªõi [3], [3] d·∫´n t·ªõi [4] v√† [4] ch·ªâ ra BPE ƒë∆∞·ª£c d√πng trong [5]: Google SentPiece.


[0] https://www.analyticsvidhya.com/blog/2020/05/what-is-tokenization-nlp/

[1] https://colah.github.io/posts/2014-07-NLP-RNNs-Representations/

[2] https://paperswithcode.com/method/bpe

[3] https://leimao.github.io/blog/Byte-Pair-Encoding

[4] https://en.wikipedia.org/wiki/Byte_pair_encoding

[5] https://github.com/google/sentencepiece

[6] https://jacky2wong.medium.com/understanding-sentencepiece-under-standing-sentence-piece-ac8da59f6b08

[7] https://gist.github.com/atinsood/6d185dfe025cbb5d55f158d4d17bc142