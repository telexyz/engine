https://twitter.com/omarsar0/status/1407404487006883842

Developing algorithms from scratch is so underrated in machine learning education.

In my experience, this is one of the first best ways to get exposed to ML. 

There is no better way to learn a concept than to try to develop it yourself and understand it at the same time.

You can find inspiration to get this experience in a lot of ML textbooks. 

In the past, I have implemented k-NNs, SVMs, PCA, Q-learning, Naive Bayes, Logistic regression, backpropagation, linear regression, and other well-known algorithms. Give it a try. Challenge yourself.

Also, a lot of the great ML professors and researchers that I have come to know over the years actually started learning this way. I was fortunate that my lecturers encouraged me to learn this way.

- - -

!!! ALGOS SHOULD BE HAND-MADE IN JULIA !!!

!!! LOW-LEVEL PROCESSING (TKNZ ...), ONLINE / NEED-FOR-SPEED ALGOS SHOULD BE IMPLEMENTED IN ZIG / RUST / C/C++ ASM, GPU ... !!!

For example, training part should be Julia, while decoding part should be Zig

- - -

https://github.com/chengchingwen/BytePairEncoding.jl

https://towardsdatascience.com/deep-learning-with-julia-flux-jl-story-7544c99728ca

https://towardsdatascience.com/n-gram-language-models-af6085435eeb


Julia for Text https://github.com/JuliaText/TextAnalysis.jl

* Container type for *Document* and *Corpus*
* DocumentTermMatrix and TF/IDF
* LSA/LDA
* Vocabulary and statistical Language Model
* Co-occurance matrix
* NaiveBayes classifier
* ROUGE evaluation metrics