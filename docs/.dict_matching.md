Hiện tại mọi syllables đều encode thành `u16` và còn dư `13_455 slots` để chứa OOS (out of syllables) nên mọi tokens đều có thể được encode thành `u16`. Điều này tối ưu cho việc lưu trữ và matching (sử dụng hashing hoặc trie).

Note: hiện tại chú trọng vào âm tiết và sau này mở rộng ra tokens, ưu tiên các lưu các OOS nguyên bản trong dict trước, phần còn lại dùng BPE để handle OOV.

Bộ từ điển tiếng Việt để tách từ sẽ gồm ít nhất 02 âm tiết nhiều nhất 04 âm tiết. Với các từ từ 05 âm tiết trở lên thường là từ kép sẽ tách được thành 2 từ đơn nhỏ hơn hoặc bằng 04 âm tiết.

- - - 

Một cách biểu diễn đơn giản coi từ điển là một tập `4-grams [s0, s1, s2, s3]`, từ có 2 âm tiết nghĩa là s3, s4 = 0, từ có 3 âm tiết thì s4 = 0. Setting như vậy khi matching từ trái qua phải thì luôn ưu tiên matching từ có nhiều âm tiết hơn trước.

`4-grams = 64-bits (16 x 4)` sử dụng https://github.com/hexops/xorfilter để tăng tốc đối sánh mẫu, tổ hợp 4-grams nhiều hơn từ trong từ điển nhiều nên trường hợp không khớp có lẽ sẽ rơi vào khoảng 80%, `Xor8 has no more than a 0.3% false-positive`, như vậy nếu khớp `1k 4-grams` thì sẽ có `ba 4-grams` là false-positive. Sai số chấp nhận được và khi scoring cuối cùng thì sẽ phải đối chiếu với n-grams thật để lấy count nên sẽ biết đâu là false-positive.


__LABELING__

Để đánh dấu một token là  token thứ mấy của một từ ta dùng 4 giá trị sau:
* 0 token đầu tiên của từ trong từ điển
* 1 token thứ hai của từ trong từ điển
* 2 token thứ ba của từ trong từ điển
* 3 token thứ tư của từ trong từ điển
* ...

Khi matching có thể có nhiều cách gộp syllables thành words, hay nói cách khác 1 syllables có thể thuộc nhiều từ ta dùng `4-bits [flag0, flag1, flag2, flag3]` để đánh dấu xem token này có thể nằm ở vị trí thứ mấy của từ:
* flag_0 == 1 có thể là token đầu tiên của một từ trong từ điển
* flag_1 == 1 có thể là token thứ hai của một từ trong từ điển
* flag_2 == 1 có thể là token thứ ba của một từ trong từ điển
* flag_3 == 1 có thể là token thứ tư của một từ trong từ điển
* ...

- - -

Khi xét một `TokensChunk` có độ dài `n` để tìm ra cách nhóm sylls2words tốt nhất thì giữa token[i] và token[i+1] có phải là `word boundary` đầu tiên không thì 1<= i <= 3 và token[0]..token[i] phải thuộc từ điển. Mỗi nhát cắt được tính điểm như sau:
1/ Điểm ưu tiên độ dài từ `k_i`
2/ `log10(count(i-gram))` của từ đó, thể hiện giữa 2 từ dài bằng nhau ưu tiên từ có tần suất xuất hiện nhiều hơn.

Một từ xuất hiện ko quá 1 tỉ (9 số không), và n =64 nên k=1000 là dư lớn để luôn ưu tiên từ dài.

`ki = 2*(i-2)`
* i = 2: k_2 = 0
* i = 3: k_3 = 2
* i = 4: k_4 = 4

=> Dùng quy hoạch động là tìm được cách nhóm từ tối ưu dựa vào hàm mục tiêu. Với nhát cắt i, điểm tốt nhất cho phần còn lại (n-i) là `k4 * (n-i/4) + switch (@rem(n-i,4)) { 3 => k3, 2 => k3, else => 0 }` (một hàm heuristic để cut branch, dạng A-star search). Một hàm heristic như vậy là không thực tế vì điểm ước lượng quá cao, ko sát với thực tế.

=> Khi thêm một ưu tiên nữa số token bị bỏ rơi ít nhất có thể => Sử dụng lại hàm mục tiêu trên thêm điểm trừ nếu 1 token bị bỏ rơi thì -10_000 điểm. Thì cách ước lượng điểm còn lại trở nên dễ dàng hơn vì số token bị bỏ rơi có thể xác định rõ ràng được (đếm số token có `sum(flag_i)=0` là biết)

- - -

Beam (bread first) vs A* (depth first) Heuristic Search
http://www.phontron.com/slides/nlp-programming-en-13-search.pdf

https://towardsdatascience.com/intuitively-understanding-connectionist-temporal-classification-3797e43a86c

- - -

[1] Thống kê từ điển thấy rằng từ tiếng Việt bao gồm: 
* 16% một âm tiết
* 71% hai âm tiết
* 13% là 3+ âm tiết
Nếu bỏ từ một âm tiết, thì số lượng 3+ âm tiết chiếm khoảng 15% (13 / 84)

[2] Thống kê file `data/VnVocab.txt`
* 28_522 từ 2 âm tiết
*  2_320 từ 3 âm tiết
*  2_831 từ 4 âm tiết
*    424   +5 âm tiết (phần lớn là thành ngữ, có thể tách nhỏ)
Số lượng 3+ âm tiết chiếm 15.1% (5.1k / 33.6k)

[3] Thống kê file [`data/wordlist.txt`](https://github.com/binhvq/vietdict106k)
* 64_220 từ 2 âm tiết
* 14_786 từ 3 âm tiết
* 10_258 từ 4 âm tiết
*  3_555   +5 âm tiết
Số lượng 3+ âm tiết chiếm 27.7% (25k / 90k)