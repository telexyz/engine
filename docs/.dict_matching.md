Hiện tại mọi syllables đều encode thành `u16` và còn dư `13_455 slots` để chứa OOS (out of syllables) nên mọi tokens đều có thể được encode thành `u16`. Điều này tối ưu cho việc lưu trữ và matching (sử dụng hashing hoặc trie).

Note: hiện tại chú trọng vào âm tiết và sau này mở rộng ra tokens, ưu tiên các lưu các OOS nguyên bản trong dict trước, phần còn lại dùng BPE để handle OOV.

Bộ từ điển tiếng Việt để tách từ sẽ gồm ít nhất 02 âm tiết nhiều nhất 04 âm tiết. Với các từ từ 05 âm tiết trở lên thường là từ kép sẽ tách được thành 2 từ đơn nhỏ hơn hoặc bằng 04 âm tiết.

- - - 

Như vậy mọi từ đều có thể biểu diễn được bằng 02 cặp n-grams `s0-s1 s2-s3` trong đó cặp `s0-s1` luôn có mặt, cặp `s2-s3` là optionals, tức là s3 có thể = 0 hoặc cả s2, s3 đều = 0.

Để matching syllables với từ điển, ta đi interate từng cặp 02 âm tiết, hệt như lúc (pair bi-gram vậy), sau khi đã match `s0-s1` rồi ta mở rộng bằng cách match tiếp cặp `s2-s3`, ưu tiên matching từ dài trước, ko được mới matching từ ngắn.

Để hỗ trợ việc matching như trên, từ được biểu diễn bởi cấu trúc dữ liệu `AutoHashMap(key, value)`, trong đó `key` là `BiGram`, `value` là a list of `BiGrams`. `key` dùng để matching `s0-s1` còn `value` là để mở rộng cặp `s2-s3`. Làm như vậy tốc độ matching sẽ vô cùng nhanh vì khoảng 15% trường hợp [1] là cần matching thêm cặp `s2-s3`.

- - -

Một cách biểu diễn đơn giản hơn là coi từ điển là một tập `4-grams [s0, s1, s2, s3]`, từ có 2 âm tiết nghĩa là s3, s4 = 0, từ có 3 âm tiết thì s4 = 0. Setting như vậy khi matching từ trái qua phải thì luôn ưu tiên matching từ có nhiều âm tiết hơn trước.

__LABELING__

Để đánh dấu một token là  token thứ mấy của một từ ta dùng 4 giá trị sau:
* 0 token đầu tiên của từ trong từ điển
* 1 token thứ hai của từ trong từ điển
* 2 token thứ ba của từ trong từ điển
* 3 token thứ tư của từ trong từ điển

Khi matching có thể có nhiều cách gộp syllables thành words, hay nói cách khác 1 syllables có thể thuộc nhiều từ ta dùng `4-bits [flag0, flag1, flag2, flag3]` để đánh dấu xem token này có thể nằm ở vị trí thứ mấy của từ:
* flag0 == 1 có thể là token đầu tiên của một từ trong từ điển
* flag1 == 1 có thể là token thứ hai của một từ trong từ điển
* flag2 == 1 có thể là token thứ ba của một từ trong từ điển
* flag3 == 1 có thể là token thứ tư của một từ trong từ điển

- - -

Khi xét một `TokensChunk` có độ dài `n` để tìm ra cách nhóm sylls2words tốt nhất thì giữa token[i] và token[i+1] có phải là `word boundary` đầu tiên không thì i <= 3 và token[0]..token[i] phải thuộc từ điển. Mỗi nhát cắt được tính điểm như sau:
1/ Điểm độ dài từ `(i' * k`, k là hằng số thể hiện độ ưu tiên độ dài từ
2/ `log10(count(i-gram))` của từ đó, thể hiện giữa 2 từ dài bằng nhau ưu tiên từ có tần suất xuất hiện nhiều hơn.

Một từ xuất hiện ko quá 1 tỉ (9 số không), và n <= 64 nên k = 1000 là đủ lớn để ưu tiên từ dài.

`i' = 3^(i-2)`
* i = 2 => i' = 1 => k2 = 1000
* i = 3 => i' = 3 => k3 = 3000
* i = 4 => i' = 9 => k4 = 9000

=> Dùng quy hoạch động là tìm được cách nhóm từ tối ưu dựa vào hàm mục tiêu. Với nhát cắt i, điểm tốt nhất cho phần còn lại (n-i) là `k4 * (n-i/4) + switch (@rem(n-i,4)) { 3 => k3, 2 => k3, else => 0 }` (một hàm heuristic để cut branch, dạng A-star search)


Thên một ưu tiên nữa số token bị bỏ rơi ít nhất có thể => Sử dụng lại hàm mục tiêu trên thêm điểm trừ 1 token bị bỏ rơi thì -10_000 điểm.

- - -

Beam (bread first) vs A* (depth first) Heuristic Search
http://www.phontron.com/slides/nlp-programming-en-13-search.pdf



- - -

[1] Thống kê từ điển thấy rằng từ tiếng Việt bao gồm: 
* 16% một âm tiết
* 71% hai âm tiết
* 13% là 3+ âm tiết
Nếu bỏ từ một âm tiết, thì số lượng 3+ âm tiết chiếm khoảng 15% (13 / 84)

[2] Thống kê file `data/VnVocab.txt`
* 28_522 từ 2 âm tiết
*  2_320 từ 3 âm tiết
*  2_831 từ 4 âm tiết
*    424   +5 âm tiết (phần lớn là thành ngữ, có thể tách nhỏ)
Số lượng 3+ âm tiết chiếm 15.1% (5.1k / 33.6k)

[3] Thống kê file [`data/wordlist.txt`](https://github.com/binhvq/vietdict106k)
* 64_220 từ 2 âm tiết
* 14_786 từ 3 âm tiết
* 10_258 từ 4 âm tiết
*  3_555   +5 âm tiết
Số lượng 3+ âm tiết chiếm 27.7% (25k / 90k)