# Machine Learning for Language Model
https://www.marekrei.com/teaching/mllm

* Implement a bigram (or higher) language model with “stupid” backoff.
  “Stupid” backoff is the easiest option that works well enough.
 
* Finish a neural network language model. Java skeleton code will be provided, and you need to fill out the parts that perform feedforward and backpropagation passes through the network. 

* Alternatively, you can implement a neural language model from scratch, in any language you wish.


# RELATED

## Constructing and Evaluating Word Embeddings
https://www.marekrei.com/teaching/cewe2017

## Unsupervised Error Detection
https://www.marekrei.com/teaching/acs2019

In this project, we will explore fully unsupervised error detection, using only unannotated corpora and methods that can also be applied to other languages where no error detection corpora are available.

* Pre-trained contextual word representations (BERT, ELMo, Flair, etc).
* Language models (GPT-2, LSTM-LM, Kneser-Ney, etc).
* Different methods for constructing synthetic data.
* Word and phrase occurrence statistics in different corpora.
* Features from other existing tools (POS taggers, parsers).

Language model based grammatical error correction without annotated training data. 
https://aclanthology.org/W18-0529.pdf

Context is key: Grammatical error detection with contextual word representations. 
