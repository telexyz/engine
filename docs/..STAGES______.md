# Các giai đoạn chiến lược để hoàn thiện engine

Telexyz nên theo hướng data-centric https://github.com/HazyResearch/data-centric-ai

Làm tốt dần / mịn dần từng bước một, có sự tham gia của con người ...

Lặp đi lặp lại theo hình xoán ốc đi lên


## stage-1: Tokenizer cho TV đã gần xong (còn OOV)

## stage-2: Syllabling là các chơi chữ, có nghĩa là tổng hợp các âm tiết thành từ
stage-2 đã lên được một KIẾN TRÚC NỀN TẢNG sử dụng chung cho nhiều modules (và cả modules mở rộng để sử dụng các modules thuộc kiến trúc nền tảng.)


__Note__: Có 2 cách xây dựng LM: n-gram là đơn giản nhất, phức tạp với độ chính xác cao là DL cần phần cứng và mềm chuyên dụng. Chốt sử dụng n-gram trước để impl và triển khai đơn giản, hiệu quả vừa đủ dùng.

- - -

# Trong khi xây dựng engine, thử nghiệm trên 2 mảng ứng dụng:


## 1/ Bộ gõ telex cải tiến để gõ song ngữ dễ dàng hơn

Làm bộ gõ native, đa nền tảng
=> Tham khảo bộ gõ https://github.com/tuyenvm/OpenKey

## 2/ Sửa lỗi, phân đoạn, tìm topics cho từng đoạn của 150 bài pháp


# YOU SHOULD REWRITE

_Viết lại modules quan trọng từ C sang Zig để hiểu thuật toán và nhuần nhuyễn Zig_

* _BPE sub-word_ https://github.com/telexyz/tools/tree/master/vocab/fastBPE/fastBPE

BPE quan trọng trong việc chia nhỏ OOV và ánh xạ OOV về một tập tokens có số lượng định trước, nhờ đó kiểm soát tốt số lượng từ vựng, hợp với việc huấn luyện mô hình có tài nguyên hạn chế.

* _Word2Vec_ https://github.com/zhezhaoa/ngram2vec/blob/master/word2vec/word2vec.c

Một module quan trọng trong việc trình bày lại token dưới dạng vector trong không gian khoảng 300 chiều, quan trọng trong việc tìm kiếm token giống nhau, dùng để train NN/LM, re-raking, re-scoring ...

https://aegis4048.github.io/optimize_computational_efficiency_of_skip-gram_with_negative_sampling