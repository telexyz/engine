# VIETNAM TKNZ

* Rule #1: SYLLABLES are FIRST CLASS Citizens
* Rule #2: SubWord must re-use known SYLLABLES
* Rule #3: Pháº£i tÃ¡ch Ä‘Æ°á»£c tokens bá»‹ dÃ­nh
	=> Cháº¥p nháº­n nhiá»u á»©ng cá»­ viÃªn tiá»m nÄƒng, chá»n lá»c sau!
* Rule #4: Don't try to be clever
* Rule #5: Prefer simple solution!

## Phase-1.1: Insights & Enhancement !!!

 16K _syllower_ 2^14
262K `alphmark` 2^18
524K `alphabet` 2^19
524K `nonalpha` 2^19

### 3rd tries

https://github.com/s-yata/marisa-trie

```
libtool: install: /usr/bin/install -c .libs/libmarisa.0.dylib /usr/local/lib/libmarisa.0.dylib
libtool: install: (cd /usr/local/lib && { ln -s -f libmarisa.0.dylib libmarisa.dylib || { rm -f libmarisa.dylib && ln -s libmarisa.0.dylib libmarisa.dylib; }; })
libtool: install: /usr/bin/install -c .libs/libmarisa.lai /usr/local/lib/libmarisa.la
libtool: install: /usr/bin/install -c .libs/libmarisa.a /usr/local/lib/libmarisa.a
libtool: install: chmod 644 /usr/local/lib/libmarisa.a
libtool: install: ranlib /usr/local/lib/libmarisa.a
```

### Data-pipeline

Máº£ng tokens[i] Ä‘á»ƒ trá» tá»›i input_bytes lÃ  1 sá»± lÃ£ng phÃ­ vÃ¬ 1 con trá» máº¥t 64 bits (8-bytes), tÆ°Æ¡ng Ä‘Æ°Æ¡ng vá»›i 1 chuá»—i ascii 8 kÃ½ tá»±. LÆ°á»£ng bá»™ nhá»› nÃ y nÃªn dÃ¹ng Ä‘á»ƒ ghi dá»¯ liá»‡u Ä‘áº§u ra theo kiá»ƒu tuyáº¿n tÃ­nh (cÃ³ thá»ƒ iterate tá»« Ä‘áº§u tá»›i cuá»‘i, cÃ³ thá»ƒ iterate chen ngang nhÆ°ng ko biáº¿t rÃµ vá»‹ trÃ­ cá»§a tokens[i]).

CÃ¡ch nghÄ© text lÃ  1 chuá»—i cá»§a tokens lÃ  Ä‘Ãºng vá» máº·t abstract nhÆ°ng khi thá»ƒ hiá»‡n má»™t cÃ¡ch cá»©ng nháº¯c ráº±ng chuá»—i tokens lÃ  1 máº£ng tokens[i] lÃ  chÆ°a thá»±c sá»± hiá»ƒu rÃµ memory cost cho viá»‡c Ä‘Ã³.

CÃ¡ch hÃ¬nh dung tá»‘t hÆ¡n lÃ  mappings. Text input lÃ  1 máº£ng bytes, báº¥t ká»³ thao tÃ¡c nÃ o (segment, tokenize, syllablize, BPE ...) Ä‘á»u lÃ  1 phÃ©p Ã¡nh xáº¡ máº£ng tuyáº¿n tÃ­nh input bytes thÃ nh máº£ng tuyáº¿n tÃ­nh output bytes.

Äáº·c tÃ­nh cá»§a máº£ng tuyáº¿n tÃ­nh lÃ  cÃ³ thá»ƒ iterate tá»« Ä‘áº§u tá»›i cuá»‘i, cÃ³ thá»ƒ iterate chen ngang tá»« báº¥t kÃ¬ vá»‹ trÃ­ bytes nÃ o nhÆ°ng ko biáº¿t rÃµ vá»‹ trÃ­ cá»§a tokens[i]. Máº£ng tuyáº¿n tÃ­nh sá»­ dá»¥ng delimiters (\s cháº³ng háº¡n), meta-data token_attrs byte cháº³ng háº¡n, hoáº·c ghi trÆ°á»›c Ä‘á»™ dÃ i cá»§a token vÃ o Ä‘áº§u token Ä‘Ã³ Ä‘á»ƒ cÃ³ thá»ƒ iterate nhanh hÆ¡n lÃ  láº§n tá»«ng byte-by-byte.

Do cÃ³ thá»ƒ chen ngang nÃªn máº£ng tuyáº¿n tÃ­nh cÃ³ thá»ƒ Ä‘Æ°á»£c cáº¯t nhá» Ä‘á»ƒ xá»­ lÃ½ song song, viá»‡c merge káº¿t quáº£ thá»±c sá»± Ä‘Æ¡n giáº£n, tháº­m chÃ­ cháº³ng cáº§n merge. NÃªn cáº¯t vÃ  lÆ°u ra nhiá»u files nhá» thÃ¬ tá»‘t hÆ¡n lÃ  lÆ°u vÃ o 1 file lá»›n. (dá»… quáº£n, dá»… xá»­ lÃ½ song song ...)

Vá»›i gÃ³c nhÃ¬n cáº£ input vÃ  output Ä‘á» lÃ  stream nhÆ° trÃªn hÃ£y tÃ¬m hiá»ƒu thÃªm vá»  https://en.wikipedia.org/wiki/Streaming_algorithm Ä‘á»ƒ lÃ m sao cho 1 láº§n scan data lÃ m Ä‘Æ°á»£c nhiá»u thá»© 1 cÃ¡ch hiá»‡u quáº£ nháº¥t!

### Bottle neck

For now the bottle neck is at HashMap tokens into types and count
By skipping hashing function in text.countToken it took 0.24 mins to segment ~600mb
with hashing function on every token it took  0.44 mins (~2x slower)

**Solution-2**: Improve hashing algorithm ... how? 
Can use perfect hashing and hash function for short input string
Or using other data structs like trie, ...

**Solution-3**: Break Text into n-parts and run each part in parallels (no-need to run
             text_utils.telexifyAlphabetTokens in a separate thread).
             After that merge n-parts' results into one! (map-reduce)

=> Solution-3 is the best choice since it apply a general pattern (map-reduce) that scale very well in both multi-threads, multi-processes or distributed-processes

=> Solution-2 is complement to solution-3 since it will speedup each single pileline

TRá» Láº I Gá»C Rá»„ Váº¤N Äá»€:

HashMap Ä‘Æ°á»£c dÃ¹ng vá»›i tokens Ä‘á»ƒ lÃ m xem xem token Ä‘ang xá»­ lÃ½ Ä‘Ã£ "gáº·p" hay chÆ°a? Náº¿u gáº·p rá»“i thÃ¬ tÄƒng count cá»§a type tÆ°Æ¡ng á»©ng lÃªn. Sau Ä‘Ã³ export HashMap keys ra thÃ¬ Ä‘Æ°á»£c list of types cÃ¹ng vá»›i counts cá»§a chÃºng. Chá»‰ Ä‘Æ¡n giáº£n váº­y thÃ´i.

Cáº£ count vÃ  types Ä‘á»u ko cáº§n chÃ­nh xÃ¡c tuyá»‡t Ä‘á»‘i vÃ¬ cuá»‘i cÃ¹ng cÃ¡i chÃºng ra dÃ¹ng lÃ  tokens lÃ  n-gram, ... vÃ  counts dá»±a trÃªn táº­p dá»¯ liá»‡u lá»›n thÃ¬ chá»‰ chÃ­nh xÃ¡c tuyá»‡t Ä‘á»‘i lÃ  ko cáº§n thiáº¿t

=> CÃ³ thá»ƒ sá»­ dá»¥ng bloom-filter (cuckoo-filter) Ä‘á»ƒ xem 1 token Ä‘Ã£ gáº·p hay chÆ°a?
=> CÃ³ thá»ƒ sá»­ dá»¥ng https://en.wikipedia.org/wiki/Count-distinct_problem Ä‘á»ƒ count

GIáº¢ Sá»¬ CHÃšNG TA CHá»ˆ QUAN TÃ‚M Tá»šI SYLLABLES

=> Thay vÃ¬ thá»±c thi trie / fsa, chÃºng ta cáº£i tiáº¿n VietSyllableParser cháº¡y nhanh nháº¥t cÃ³ thá»ƒ vÃ¬ Ä‘áº±ng nÃ o cÅ©ng pháº£i Parse vÃ  sau phi parse vá»›i má»—i Syllable ta Ä‘Æ°á»£c 1 `u17` univeral id tÆ°Æ¡ng á»©ng vá»›i tá»«ng syllower.

QUAY TRá» Láº I Gá»C Rá»„ Váº¤N Äá»€:

Convert `can_be_vietnamese tokens` => `syllower's id` + `attrs`, vá»›i cÃ¡c tokens khÃ¡c giá»¯ nguyÃªn ná»™i dung gá»‘c, chá» xá»­ lÃ½ sau ...

Nhá»¯ng nice-to-have features khÃ¡c nhÆ° types, counts ta xá»­ lÃ½ báº±ng nhá»¯ng data struct vÃ  algos ko cáº§n Ä‘á»™ chÃ­nh xÃ¡c tuyá»‡t Ä‘á»‘i => Prob Data Struct :D 

Hoáº·c lÃ  vá»›i nhá»¯ng token ngáº¯n, cÃ³ thá»ƒ sá»­ dá»¥ng bitset Ä‘á»ƒ Ä‘Ã¡nh dáº¥u xem Ä‘Ã£ gáº·p hay chÆ°a. Vá»›i token dÃ i thÃ¬ dÃ¹ng bloom filter cháº³ng háº¡n.

FINALLY: Bá» viá»‡c count qua 1 bÃªn, ta cÃ³ thá»ƒ loáº¡i bá» hoÃ n toÃ n viá»‡c dÃ¹ng HashMap khi xá»­ lÃ½ token báº±ng cÃ¡ch:

1/ `syllable tokens` => `syllower's id` + `attrs` rá»“i Ä‘áº¿m `syll_id`
2/ Vá»›i token ngáº¯n (5-bytes) sá»­ dá»¥ng bitset
3/ Vá»›i token dÃ i sá»­ dá»¥ng bloom filter ....

NOTE: Vá»›i má»—i token hiá»‡n táº¡i ta Ä‘Ã£ phÃ¢n loáº¡i Ä‘Æ°á»£c xem token Ä‘Ã³ thuá»™c báº£ng chá»¯ cÃ¡i hay ko vÃ  náº¿u thuá»™c báº£ng chá»¯ cÃ¡i thÃ¬ cÃ³ thuáº§n a-z hay khÃ´ng? (`nonalpha` vs `alphamark` vs `alphabet`) Viá»‡c phÃ¢n loáº¡i cÃ³ thá»ƒ giÃºp Ã­ch cho viá»‡c count Ä‘á»¡ tá»‘n sá»©c hÆ¡n, vÃ­ dá»¥ mÃ£ hÃ³a láº¡i hoáº·c mapping vÃ o 1 táº­p bits nhá» hÆ¡n, giÃºp count / filter tá»‘t hÆ¡n cháº³ng háº¡n.

- - -


## Phase-2: Subword segmentation

https://everdark.github.io/k9/notebooks/ml/natural_language_understanding/subword_units/subword_units.nb.html#12_probablistic_subword_segmentation

We iterate over every position in a given word. At each end-of-character position, we determine the best segment by finding the one with highest likelihood given the current vocabulary.

we need to have a vocabulary for subwords that can attribute each subword to a probability. We can use BPE to build such vocabulary. For complete coverage we will also include character-level subwords into the vocabulary.

https://everdark.github.io/k9/notebooks/ml/natural_language_understanding/subword_units/subword_units.nb.html#123_em_with_viterbi

Now we know the idea of EM, and we know how to find the optimal segment path by Viterbi, we can put them together to forumlate the optimization task of our probablistic subword segmentation.

* Initialize a large seeding subword vocabulary from the training corpus
* [Expectation] Estimate each subword probability by the corresponding frequency counts in the vocabulary
* [Maximization] Use Viterbi to segment the corpus, returning the optimal segments
* Compute the loss of each new subword from optimal segments
* Shrink the vocabulary size by dropping the subwords with top X% smallest losses
* Repeat step 2 to 5 until the vocabulary size reaches a desired number

The loss of a subword in step 4 is the reduction in overall training corpus segment likelihood if that subword is removed from the current vocabulary.

we used BPE to generate the initial vocabulary. Another common choice is to the suffix array algorithm. to generate the common substrings.


```js
Input: "Nguowif", "Æ¡iii", "chaof", "nhes", "!"
Output: "Nguowif", "owi", "i", "i", "chaof", "nhes", "!"
```
After phase-#1, the number of others-tokens is quite small. For above example we need to process token number 3 only!


TODO:

* Sá»­ dá»¥ng tokens dáº¡ng nguyÃªn báº£n utf8 cá»§a vocab Ä‘á»ƒ scan OOV types, nháº±m tÃ¡ch cÃ¡c thÃ nh pháº§n cháº¯c cháº¯n lÃ  Ã¢m tiáº¿t. ChÃº Ã½ cÃ¡ch phÃ¢n biá»‡t chá»¯ cÃ¡i viáº¿t hoa vs chá»¯ cÃ¡i viáº¿t thÆ°á»ng cÃ³ thá»ƒ dÃ¹ng Ä‘á»ƒ lÃ m boundary Ä‘á»ƒ tÃ¡ch (xem vÃ­ dá»¥ dÆ°á»›i).

* CÃ³ nhiá»u cÃ¡ch tÃ¡ch thÃ¬ giá»¯ láº¡i thÃ nh nhiá»u dá»‹ báº£n, cÃ ng nhiá»u á»©ng cá»­ viÃªn tiá»m nÄƒng cÃ ng tá»‘t, cÃ¢n háº¿t!

* Choose an effective subword tknz algo that suitale for Vi and reuse syllable vocab

```js
Try: "Máº¹HÃ My" -> "Máº¹HÃ M|y" -> true
Try: "Máº¹nuÃ´i" -> "Máº¹n|uÃ´i" -> true
Try: "NgheNh|Ã¬n" -> false
Try: "MÃ´n|Ã´lÃ´xá»‘p" -> "MÃ´|nÃ´|lÃ´|xá»‘p" ?? nÃªn giá»¯ nguyÃªn vÃ¬ Ä‘Ã¢y lÃ  tÃªn riÃªng
```

This phase must:

* Re-use syllable vocab to break tokens in meaningful parts

* Combining subword tokenize techniques to obtain best results


- - -

## Phase-1: Space-splitter (space-32, tab-9) and alphabet vs nonalpha splitter

[ LATER ] <= NOT THE BOTTLE NECK

* Find an efficient way to check if a _utf8 type_ is a Vi Syllable or not!
  - We know that `am_giua` is a must for a Vi Syllable
  - Some `am_giua` can follow one some `am_dau` and only be followed by some `am_cuoi`

* Test syllable counts

* Sort type counts before write to files

__TODOs__:

[ DONE ]

* Reject mixed upper vs lower case syllable, keep only titelized or capitalized sylls

* Handle `Thoá»ng`: need to convert `oo` to `ooo` before passing to syll-parser. 
`oÃ´`, `Ã´o` are invalid, can recognized by using `char_stream.has_mark` 

* Test types' counts

* More test cases for `telexify.zig` and `src/text_data_struct.zig`

* Break alphabet-types into have-marks-and-tone vs others
FIX: why "cp" is categoried as alphmark?

* colapse types that have similar _ascii-telex trans_ to create next layer of _ascii-telex types_

* Convert _utf8 types_ to _ascii-telex types_

* Choose an efficient bytes-hashing algorithm to count similar _utf8 lower_case-fom tokens_ to create _utf8 types_ (see `_HASH.md`)

### One-Way-Mapping Multiple layer of (Re-)presentations:
1. utf8 bytes => 2.
2. utf8 non-breaking tokens => 3.
3. utf8 alphabet tokens & utf8 delimiter (non-alphabet) tokens => 4.
4. utf8 alphabet types => 5. & utf8 delimiter (non-alphabet) types
5. ascii-telex syllable types (syllable vocab)

The purpose of this phase is to split the input text into a list of non-breaking tokens separated by space characters that can be inputted from keyboard (32, 9).

It's could be Syllables `ChÃ¢y   Ã¬   ná»™p   pháº¡t   nguá»™i   ChÃ¡u   Ä‘Ã²i   tiá»n   cÆ¡m   dÃ¬   nhÃ    ÄÃ    Náºµng   nghiÃªn   cá»©u   tiá»‡n   Ã­ch   nháº¯n   tin   khi   vi   pháº¡m   Ä‘áº¿n   chá»§   phÆ°Æ¡ng   KhÃ³   xá»­   vá»¥   máº¹   tuá»•i   trá»™m   xe   hÆ¡i   cá»§a   con   gÃ¡i   Thay   Ä‘á»•i   vá»   Ä‘Äƒng   kÃ½   chuyá»ƒn   nhÆ°á»£ng   tá»«   báº¡n   cáº§n   biáº¿t   Nhá»¯ng   trÆ°á»ng   há»£p   trÆ°ng   cáº§u   giÃ¡m   Ä‘á»‹nh   trong   Ã¡n   kinh   táº¿   Thá»‹   tráº¥n   á»Ÿ   bÃ¡n   vá»›i   giÃ¡   hÆ¡n   Ä‘á»ƒ   thu   hÃºt   cÆ°   dÃ¢n   Bá»   quy`

Or non-alphabet / abbr / foreign-words ... `.   ,   70   12/2   1   12/2/2018:   20   :   '   '.   12/2.   20.000   02/2018.   2/2018?.   2/2018.   12/2:   24.000   2   ?.   -   12/02/2018,   18   (   ).   12   !'.   12:   7/18.   12/2/2018.   m2   BOT   18   QL18   TPHCM   CAND   7   FLC   4   SEA   Games   PVP   Land   U23   6km   MC   68   3   Samsung   Display   300   VFF   29   8   TNCN   AFF   Cup   2008   23   Italy   euro   200   Vietlott   105   27   21   casino   1986   FDI   jeans   DNNVV   bikini   TP   HCM   25   30   Rolls   Royce   Bespoke   2017   Cagliari   Juventus   HLV   Allegri   Serie   Icardi   Inter   80   4000   26   Rome   Mourinho   Morata   C1   Real   Ronaldo   VN   K   BHXH   THPT   Myanmar   Rohingya   TAND   T   ara   Facebook   Clip   Mercedes   container   Venezuela   265   Google   Uber   Aerobic   260   16   Malaysia   Chol`,

Or look-like-Vietnamese (typo, abbr, borrowed-words ...) `BHYTNÃ¢ng   ÄHTB   BÃ´lykhÄƒmxay   nÃ y15   XÃ´viáº¿t   iá»‘t   Ná»™iThÃ­ch   HÄBA   crÃ´m   ChilÃª   HÄLÄ   uyÃªnhXÃ­   HrÃª   KrÃ´ng   BÄKH   Ä‘Ã´la   ÄHQG   EurÃ©ka   QSÄ   Ä‘Ã³nTáº¿t   Tiáº¿g   toÃ ndiá»‡n   zÃ¡o   zá»¥k   ÄT601   LÄLÄVN   LÄTB   zÄƒng   CQÄT   Ä‘Ã´lÃ´mit   Thoá»ng   Váº¯cxin   ÄVHD   Ã¡Bo   PTÄ   CÄCS   xtÃª   GÄKT   kÃªt   sÆ¡mi   QÄND   ATVSLÄ   MÃ´t   háº¡iBÃ i`


This phase must:

* Moving fast and try to detect as much syllable-tokens using strict rules [$] as possible!

* Convert utf-8 syllable-token to ascii-telex syllable-token

* Treat newline-10 as a special token type

* Adding class-attribute to each token 1 => Syllable, 2 => Newline 3 => Others

* Counting similar tokens to create types

[$] Strick rules ensure that only utf-8 token that 100% look like a vi-syllable with 0-confusion is converted. VD: `NgÆ°Æ¡Ã¬ => Nguwowif` but not confusing case like `ngÆ°á»Ã­` or `cÃ¡iiii gÃ¬????` ...

```js
Input: "NgÆ°Æ¡Ã¬ Æ¡iii chÃ o nhÃ©!"
Output: "Nguowif", "Æ¡iii", "chaof", "nhes", "!"
```

By doing all of this we respect Rule #1/ that makes "SYLLABLES are FIRST CLASS Citizens" so we can build a syllable vocab, and prepare input data the next phase.

- - -

## Phase-3: Everything Else / Post-processing

Digit tknz?, Viáº¿t táº¯t? ...

- - - 

# REF: SoTA HF Tokenizer
https://huggingface.co/docs/tokenizers/python/latest/components.html

## Normalizer
* Unicode normalization algorithms (NFD, NFKD, NFC & NFKC)
* Lowercasing
* Strip: Removes all whitespace characters on the specified sides (left, right or both) of the input
* ...
* Sequence: Composes multiple normalizers that will run in the provided order 
	`Sequence([NFKC(), Lowercase()])`

## PreTokenizer

The PreTokenizer takes care of splitting the input according to a set of rules. This pre-processing lets you ensure that the underlying Model does not build tokens across multiple â€œsplitsâ€. For example if you donâ€™t want to have whitespaces inside a token, then you can have a PreTokenizer that splits on these whitespaces.

### ByteLevel Spliting (will be used with BPE)

Splits on whitespaces while remapping all the bytes to a set of visible characters. This technique as been introduced by OpenAI with GPT-2 and has some more or less nice properties:

	* Since it maps on bytes, a tokenizer using this only requires 256 characters as initial alphabet (the number of values a byte can have), as opposed to the 130,000+ Unicode characters.

	* A consequence of the previous point is that it is absolutely unnecessary to have an unknown token using this since we can represent anything with 256 tokens (Youhou!! ğŸ‰ğŸ‰)

	* For non ascii characters, it gets completely unreadable, but it works nonetheless!


## BPE

One of the most popular subword tokenization algorithm. The Byte-Pair-Encoding works by starting with characters, while merging those that are the most frequently seen together, thus creating new tokens. It then works iteratively to build new tokens out of the most frequent pairs it sees in a corpus.

BPE is able to build words it has never seen by using multiple subword tokens, and thus requires smaller vocabularies, with less chances of having â€œunkâ€ (unknown) tokens.


## WordPiece

This is a subword tokenization algorithm quite similar to BPE, used mainly by Google in models like BERT. It uses a greedy algorithm, that tries to build long words first, splitting in multiple tokens when entire words donâ€™t exist in the vocabulary. This is different from BPE that starts from characters, building bigger tokens as possible.

It uses the famous ## prefix to identify tokens that are part of a word (ie not starting a word).

## Unigram

Unigram is also a subword tokenization algorithm, and works by trying to identify the best set of subword tokens to maximize the probability for a given sentence. This is different from BPE in the way that this is not deterministic based on a set of rules applied sequentially. Instead Unigram will be able to compute multiple ways of tokenizing, while choosing the most probable one.


- - - 

## Blah, blah, blah ...

https://blog.floydhub.com/tokenization-nlp/

TOKEN, TOKENIZE, TOKENIZATION: WHAT'S TOKEN? WHY TOKENIZATION? TOKENIZE FOR WHAT?

@SÃ¡ng 15/06/2021 trong lÃºc chuáº©n hoÃ¡ cÃ¡c viá»‡c sá»­ dá»¥ng `terms` (see notes_on_terms.md), chá»£t nháº­n ra mÃ¬nh Ä‘ang thiáº¿u sá»± hiá»ƒu Ä‘Ãºng vá» tháº¿ nÃ o `token`, tÃ¬m kiáº¿m trÃªn Google vÃ  tÃ¬m Ä‘áº¿n [0]. BÃ i viáº¿t chÃ¢n phÆ°Æ¡ng, dá»… hiá»ƒu khiáº¿n mÃ¬nh nháº­n ra:

Token hiá»ƒu Ä‘Æ¡n giáº£n lÃ  Ä‘áº§u ra cá»§a quÃ¡ trÃ¬nh biáº¿n Ä‘á»•i cÃ¡c cÃ¢u chá»¯ Ä‘áº§u vÃ o (text) thÃ nh cÃ¡c Ä‘Æ¡n-vá»‹ ná»n táº£ng Ä‘á»ƒ lÃ m cÆ¡-sá»Ÿ cho cÃ¡c bÆ°á»›c xá»­ lÃ½ tiáº¿p theo.

NhÆ° váº­y Ä‘á»‹nh nghÄ©a vá» `token`, `tokenize` ... lÃ  rá»™ng, khÃ´ng cá»‘ káº¿t vá»›i báº¥t cá»© thá»© gÃ¬ cá»¥ thá»ƒ. CÃ¡ch hiá»ƒu tokenization lÃ  tÃ¡ch-tá»« nhÆ° mÃ¬nh váº«n cÃ³ trong Ä‘áº§u lÃ  bá»‹ thiÃªn lá»‡ch, chÆ°a tháº¥u Ä‘áº¡t vÃ  thoáº£ Ä‘Ã¡ng.

BÃ i viáº¿t gá»£i ra 1 tá»« khoÃ¡ lÃ m mÃ¬nh tÃ² mÃ² `Subword tokens` (Character tokens: s-m-a-r-t-e-r vs Subword tokens: smart-er) vÃ¬ tá»« trÆ°á»›c tá»›i nay mÃ¬nh chÆ°a Ä‘Ã o sÃ¢u mÃ  chá»‰ hiá»ƒu nÃ´m na tokenization lÃ  tÃ¡ch-tá»«, mÃ  giá» cÃ²n tÃ¡ch-nhá»-hÆ¡n-cáº£-tá»« ná»¯a cÆ¡ Ã ? 

Thá»±c ra tÃ¡ch nhá» hÆ¡n tá»« lÃ  chuyá»‡n thÆ°á»ng nhÆ° cÃ¢n Ä‘Æ°á»ng há»™p sá»¯a, tiáº¿ng Anh cÃ³ prefix, suffix, root ..., tiáº¿ng Viá»‡t cÃ³ cÃ¡ch phÃ¢n tÃ¡ch Ã¢m tiáº¿t nhÆ° sau (thÃ¬ chÃ­nh lÃ  subword rá»“i cÃ²n gÃ¬ :^)
```js
Ã¢m tiáº¿t = phá»¥ Ã¢m Ä‘áº§u + váº§n
váº§n = Ã¢m giá»¯a + Ã¢m cuá»‘i + thanh Ä‘iá»‡u
Ã¢m giá»¯a = Ã¢m Ä‘á»‡m + nguyÃªn Ã¢m chÃ­nh

Note: nguyÃªn Ã¢m chÃ­nh luÃ´n cÃ³ vÃ  thanh Ä‘iá»‡u luÃ´n cÃ³
Phá»¥ Ã¢m Ä‘áº§u, Ã¢m Ä‘á»‡m, vÃ  Ã¢m cuá»‘i cÃ³ thá»ƒ ko cÃ³. 
Váº§n luÃ´n cÃ³ vÃ  thanh Ä‘iá»‡u lÃ  1 thuá»™c tÃ­nh cá»§a váº§n, bao trÃ¹m toÃ n bá»™ váº§n

VD1: cÃ¡ch phÃ¢n tÃ¡ch theo phÃ¡t Ã¢m

gáº·t		g {a2 t}_T6
chÆ°a	ch ua2_T1
Ä‘Æ°á»£c	d2 {ua2 k}_T6
hÃ¬nh	h {i nh}_T2
thÃ nh	th {a1 nh}_T2
thÃ³i	th oi_T3
quen	k {u1 e1 n}_T1

VD1: cÃ¡ch phÃ¢n tÃ¡ch theo nháº­p liá»‡u TELEX tá»« bÃ n phÃ­m
Quy Æ°á»›c bá» dáº¥u ngay sau nguyÃªn Ã¢m vÃ  thanh Ä‘iá»‡u gÃµ cuá»‘i cÃ¹ng

gáº·t		g aw t j
chÆ°a	ch uwa
Ä‘Æ°á»£c	dd uwow c j
hÃ¬nh	h i nh f
thÃ nh	th a nh f
thÃ³i	th oi s
quen	qu e n

```

MÃ¬nh vá»«a lÃ m xong module encode Ã¢m tiáº¿t tiáº¿ng Viá»‡t thÃ nh `u17` (dÃ¹ng 17-bits) trong Ä‘Ã³ {phá»¥ Ã¢m Ä‘áº§u} cáº§n `u5`, {nguyÃªn Ã¢m Ä‘á»‡m + nguyÃªn Ã¢m chÃ­nh} cáº§n `u5`, {Ã¢m cuá»‘i} cáº§n `u4` vÃ  {thanh Ä‘iá»‡u} cáº§n `u3` nÃªn Ã½ thá»©c rÃµ vá» subwords trong tiáº¿ng Viá»‡t, cá»™ng thÃªm cÃ¡c kiá»ƒu biáº¿n thá»ƒ viáº¿t táº¯t, tiáº¿ng vÃ¹ng miá»n, viáº¿t kiá»ƒu "tráº» trÃ¢u" trÃªn internet ná»¯a thÃ¬ subwords "zÃ´ cÃ¹ngg Ä‘a ja.ng vÃ  thÃº vá»‹á»‹á»‹á»‹á»‹á»‹á»‹". 

Náº¿u chá»‰ thÃ´ng qua chuáº©n hoÃ¡ Ä‘áº§u Ä‘á»ƒ quy chuáº©n cÃ¡c biáº¿n thá»ƒ Ä‘Ã³ thÃ nh Ã¢m tiáº¿t gá»‘c thÃ¬ Ä‘á»i cÃ²n gÃ¬ lÃ  "vÃ´ cÃ¹ng Ä‘a dáº¡ng vÃ  thÃº vá»‹" ná»¯a :'( Bá»Ÿi chÃºng ta Ä‘Ã£ lÃ m máº¥t Ä‘i nhiá»u thÃ´ng tin khÃ¡c mÃ  ngÆ°á»i gÃµ muá»‘n chuyá»ƒn táº£i qua con chá»¯ nhÆ° lÃ  tÃ¢m tráº¡ng, vÃ¹ng miá»n, hay Ä‘Æ¡n giáº£n lÃ  "gÃµ kÃ©m", "lÃ³ng ngÃ³ng", "máº¯t má», tay cháº­m", "chÃ­nh táº£ dá»‘t" nÃªn gÃµ nháº§m hoÃ i!

Tiáº¿p Ä‘Ã³, bÃ i viáº¿t thá»Ÿ ra má»™t cÃ¢u chá»‘t "Transformed based models â€“ the SOTA in NLP â€“ rely on Subword Tokenization algorithms for preparing vocabulary. One of the most popular Subword Tokenization algorithm known as Byte Pair Encoding (BPE) that tackles OOV effectively."

Táº¡m dá»‹ch: CÃ¡c MÃ´-hÃ¬nh "biáº¿n Ä‘á»•i Ã¡nh xáº¡", (mÃ´ hÃ¬nh) Xá»­-LÃ½-NgÃ´n-Ngá»¯-Tá»±-NhiÃªn Ä‘Æ°Æ¡ng Ä‘áº¡i, dá»±a vÃ o cÃ¡c thuáº­t toÃ¡n `Subword Tokenization` Ä‘á»ƒ chuáº©n bá»‹ tá»« vá»±ng. MÃ£ hoÃ¡ cáº·p-byte (BPE) lÃ  thuáº­t toÃ¡n `Subword Tokenization` "quá»‘c dÃ¢n" nháº¥t, nÃ³ giáº£i quyáº¿t OOV hiá»‡u quáº£!

VD: word-embedding biáº¿n-Ä‘á»•i tokens báº±ng cÃ¡ch Ã¡nh xáº¡ nÃ³ 1-1 vá»›i cÃ¡c vectors toÃ¡n há»c cá»±c nhiá»u chiá»u (vÃ i trÄƒm cháº³ng háº¡n) [1]. 

MÃ¬nh vá»«a `tokenize` 1 copus gáº§n 600MB text tiáº¿ng Viá»‡t vÃ  OOV khÃ¡ nhiá»u nÃªn bá»“ káº¿t ngay quáº£ "giáº£i quyáº¿t OOV hiá»‡u quáº£". NÃ³i váº­y Ä‘á»§ Ä‘á»ƒ hiá»ƒu subword tokenization ráº¥t hay ho :^)

- - -

Luáº­n vá» xá»­ lÃ½ thÃ´ng tin, con ngÆ°á»i hay mÃ¡y tÃ­nh khÃ´ng pháº£i lÃºc nÃ o cÅ©ng nháº­n Ä‘Æ°á»£c tÃ­n hiá»‡u Ä‘áº§u vÃ o CHUáº¨N mÃ  hai bÃªn Ä‘Ã£ quy Æ°á»›c vá»›i nhau tá»« trÆ°á»›c. ChÃºng ta pháº£i suy Ä‘oÃ¡n (a.k.a giáº£i mÃ£) ráº¥t nhiá»u Ä‘á»ƒ cÃ³ thá»ƒ hiá»ƒu Ä‘Æ°á»£c ngÃ´n ngá»¯, trong Ä‘á»‘i thoáº¡i hÃ ng ngÃ y cháº³ng háº¡n: 
+ cÃ³ ngÆ°á»i nÃ³i ráº¥t nhanh, nÃ³i táº¯t, nÃ³i thiáº¿u tá»« (máº¥t tÃ­n hiá»‡u -  signal lost/uncompleted)
+ cÃ³ ngÆ°á»i nÃ³i rá» rÃ , á» Ã , loáº±ng ngoáº±ng (nhiá»…u tÃ­n hiá»‡u - signal vs noise)
+ cÃ³ ngÆ°á»i dÃ¹ng tiáº¿ng Ä‘á»‹a phÆ°Æ¡ng, bá»“i thÃªm tiáº¿ng nÆ°á»›c ngoÃ i (nháº­n Ä‘Æ°á»£c tÃ­n hiá»‡u nhÆ°ng thiáº¿u thÃ´ng tin vá» tÃ­n hiá»‡u Ä‘Ã³ - unknown signal)

Trong xá»­ lÃ½ tÃ­n hiá»‡u (cÃ³ liÃªn quan tá»›i mÃ¡y tÃ­nh) Ä‘Ã³ lÃ  quÃ¡ trÃ¬nh khá»­ nhiá»…u, khÃ´i phá»¥c tÃ­n hiá»‡u bá»‹ máº¥t tá»« nhá»¯ng tÃ­n hiá»‡u Ä‘Ã£ cÃ³, giáº£i mÃ£ tÃ­n hiá»‡u báº±ng nhiá»u mÃ´ hÃ¬nh khÃ¡c nhau ...

Khi gÃµ/chat cÅ©ng tháº¿, gÃµ sai chÃ­nh táº£; gÃµ táº¯t cho nhanh; gÃµ thiáº¿u dáº¥u, thiáº¿u thanh Ä‘iá»‡u; nÃ£o chÃºng ta pháº£i suy Ä‘oÃ¡n (giáº£i mÃ£) ráº¥t lÃ  nhiá»u, cÃ²n chÆ°a ká»ƒ yáº¿u tá»‘ tá»« Ä‘á»“ng Ã¢m khÃ¡c nghÄ©a, tá»« Ä‘a nghÄ©a ... ná»¯a, nÃ³ buá»™c chÃºng ta pháº£i Há»ŒC liÃªn tá»¥c: Há»c báº±ng cÃ¡ch pháº£i Náº P thÃ´ng tin chung quanh chá»§ Ä‘á» Ä‘ang trao Ä‘á»•i, báº±ng cÃ¡ch diá»…n giáº£i láº¡i vÃ  há»i Ä‘á»‘i phÆ°Æ¡ng xem mÃ¬nh hiá»ƒu nhÆ° váº­y cÃ³ Ä‘Ãºng khÃ´ng, báº±ng va cháº¡m thá»­ sai (trials and errors) Ä‘á»ƒ tá»± rÃºt kinh nghiá»‡m. Khi há»c Ä‘á»§ rá»“i thÃ¬ chá»‰ cáº§n ngÆ°á»i kia buá»™t miá»‡ng 1 cÃ¡i lÃ  mÃ¬nh biáº¿t há» Ä‘á»‹nh nÃ³i cÃ¡i gÃ¬ rá»“i, hoáº·c cho cáº­u gÃµ táº¯t thoáº£i mÃ¡i tá»› liáº¿c cÃ¡i lÃ  hiá»ƒu liá»n, hoáº·c nhÆ° trong khi chat vá»›i nhau tháº¥y bÃªn kia cá»© ngáº­p ngá»«ng, gÃµ rá»“i mÃ  chÆ°a tháº¥y gá»­i Ä‘i lÃ  Ä‘oÃ¡n há» Ä‘ang cÃ³ Ä‘iá»u gÃ¬ ngáº­p ngá»«ng khÃ³ nÃ³i Ä‘Ã¢y ... 

Tá»©c lÃ  ngÃ´n ngá»¯ chá»‰ lÃ  phÆ°Æ¡ng tiá»‡n Ä‘á»ƒ ngÆ°á»i ta truyá»n táº£i truyá»n táº£i Ã½ Ä‘á»‹nh / Ã½ muá»‘n cá»§a mÃ¬nh cho ngÆ°á»i khÃ¡c hiá»ƒu Ä‘Æ°á»£c, nÃ³ chá»‰ lÃ  phÆ°Æ¡ng tiá»‡n, lÃ  kÃªnh truyá»n táº£i khÃ´ng hÆ¡n. Trong kÃªnh truyá»n táº£i Ä‘Ã³ ngoÃ i tá»« ngá»¯, ngÃ´n ngá»¯ cÃ²n nhiá»u thÃ´ng tin ná»¯a vá» hoÃ n cáº£nh, tráº¡ng thÃ¡i cáº£m xÃºc, sá»©c khoáº», vÄƒn hoÃ¡, vÃ¹ng miá»n, thá»i tiáº¿t (láº¡nh quÃ¡ nÃ³i run láº­p cáº­p), bÃ n phÃ­m t9 (Ä‘iá»‡n thoáº¡i cá»• chá»‰ cÃ³ 10 phÃ­m 0-9 Ä‘Æ°á»£c lá»“ng ghÃ©p chá»¯ cÃ¡i a-z vÃ o Ä‘Ã³) khÃ´ng thá»ƒ gÃµ Ä‘á»§ dáº¥u Ä‘Æ°á»£c ... Ä‘Æ°á»£c truyá»n táº£i theo má»™t cÃ¡ch trá»±c tiáº¿p hoáº·c giÃ¡n tiáº¿p. Nhá»¯ng thÃ´ng tin Ä‘Ã³ giÃºp chÃºng ta (vÃ  mÃ¡y tÃ­nh) hiá»ƒu sÃ¢u hÆ¡n vá» thÃ´ng Ä‘iá»‡p muá»‘n Ä‘Æ°á»£c truyá»n táº£i, thÃ´ng cáº£m hÆ¡n vá»›i ngÆ°á»i nÃ³i / gÃµ, dá»±a vÃ o Ä‘Ã³ mÃ  pháº£n há»“i ká»‹p thá»i, Ä‘Ãºng trá»ng tÃ¢m, thoáº£ mÃ£n hai bÃªn, khiáº¿n cáº£ hai Ä‘áº¡t Ä‘Æ°á»£c tráº¡ng thÃ¡i mÃ£n nguyá»‡n. Theo mÃ¬nh nhÆ° váº­y má»›i gá»i lÃ  giao tiáº¿p cÃ³ tÃ¢m. NhÆ° váº­y má»›i lÃ  má»¥c Ä‘Ã­ch cáº§n Ä‘áº¡t Ä‘Æ°á»£c cá»§a xá»­ lÃ½ thÃ´ng tin, xá»­ lÃ½ ngÃ´n ngá»¯ ...

Viá»‡c chuáº©n hoÃ¡ tÆ°á»Ÿng lÃ  hay nhÆ°ng láº¡i lÃ m máº¥t Ä‘i cÃ¡c tÃ­n hiá»‡u mÃ  ngÆ°á»i chuáº©n hoÃ¡ cho ráº±ng lÃ  khÃ´ng cáº§n thiáº¿t, nÃ³ lÃ m cho viá»‡c giáº£i mÃ£ trá»Ÿ nÃªn Ä‘Æ¡n giáº£n hÆ¡n vá»›i ngÆ°á»i hoáº·c mÃ¡y tÃ­nh nhÆ°ng lÃ m xa rá»i Ä‘i má»¥c Ä‘Ã­ch cáº§n Ä‘áº¡t Ä‘Æ°á»£c á»Ÿ trÃªn. Khi gÃµ vÄƒn báº£n cÅ©ng tháº¿, tÃ­n hiá»‡u nguyÃªn báº£n nháº¥t mÃ  mÃ¡y tÃ­nh nháº­n Ä‘Æ°á»£c chÃ­nh lÃ  cÃ¡c tÃ­n hiá»‡u nháº­n Ä‘Æ°á»£c tá»« bÃ n phÃ­m (key-strokes), lÃ m sao Ä‘á»ƒ giá»¯ Ä‘Æ°á»£c nguyÃªn báº£n cÃ¡c key-strokes Ä‘Ã³, xá»­ lÃ½ trá»±c tiáº¿p trÃªn cÃ¡c key-strokes Ä‘Ã³ sáº½ lÃ m cho bÃ i toÃ¡n trá»Ÿ nÃªn sinh Ä‘á»™ng vÃ  há»¯u Ã­ch hÆ¡n ráº¥t nhiá»u so vá»›i viá»‡c chá»‰ coi vÄƒn báº£n lÃ  1 chuá»—i cÃ¡c kÃ½ tá»±a Ä‘Æ°á»£c mÃ£ hoÃ¡ (thuá»Ÿ xÆ°a lÃ  ascii, thá»§a nay lÃ  unicode/utf-8).

EDGE CASES MATTER!

Vá»›i gÃ³c nhÃ¬n trÃªn ngoÃ i key-strokes (chá»‰ nghi nháº­n Ä‘Æ°á»£c thÃ´ng qua bá»™ gÃµ tiáº¿ng Viá»‡t) chÃ­nh cÃ¡c OOV (out-of-vocabulary words) má»›i lÃ  thá»© sinh Ä‘á»™ng, thÃº vá»‹ Ä‘Ã¡ng Ä‘á»ƒ Ä‘Ã o sau tÃ¬m hiá»ƒu, xem thÃ´ng Ä‘iá»‡p Ä‘áº±ng sau Ä‘Ã³ lÃ  gÃ¬ ... ?

Tiáº¿ng Viá»‡t lá»ng láº»o nÃªn kháº£ nÄƒng CHÆ I chá»¯ ráº¥t nhiá»u, nÃ³i cÃ¡ch khÃ¡c lÃ  cÃ¡c biáº¿n thá»ƒ vÃ  thÃ´ng tin thÃªm Ä‘i kÃ¨m cÃ¡c biáº¿n thá»ƒ Ä‘Ã³ ráº¥t nhiá»u ...

Ã”NG CHá»¦ VÃ€ CÃ”NG DÃ‚N Háº NG NHáº¤T, Háº NG 2,3

Bá»™ gÃµ tiáº¿ng Viá»‡t, lÃ  má»™t bÃ i toÃ¡n cá»±c hay mÃ  chÆ°a Ä‘Æ°á»£c khai thÃ¡c háº¿t, vÃ¬ chá»‰ cÃ³ bá»™ gÃµ má»›i nháº­n Ä‘Æ°á»£c thÃ´ng tin tá»« bÃ n phÃ­m qua thá»i gian thá»±c má»™t cÃ¡ch nguyÃªn báº£n vÃ  tráº§n trá»¥i nháº¥t. NÃ³ giá»‘ng nhÆ° chÃºng ta nghe/há»c trá»±c tiáº¿p vÃ  cÃ³ thá»ƒ tÆ°Æ¡ng tÃ¡c há»i Ä‘Ã¡p láº¡i ngÆ°á»i Ä‘ang nÃ³i/giáº£ng vá»›i viá»‡c chÃºng ta nghe láº¡i / xem láº¡i trÃªn youtube: HoÃ n toÃ n khÃ¡c biá»‡t, vÃ  á»Ÿ má»™t Ä‘áº³ng cáº¥p khÃ¡c, ko thá»ƒ so sÃ¡nh vá»›i nhau.

NhÆ° váº­y bá»™ gÃµ chÃ­nh lÃ  cÃ´ng dÃ¢n háº¡ng nháº¥t, OOV lÃ  cÃ´ng dÃ¢n háº¡ng hai, bá»™ tá»« vá»±ng (Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a tá»« trÆ°á»›c trong tá»« Ä‘iá»ƒn) lÃ  cÃ´ng dÃ¢n háº¡ng ba. CÃ´ng dÃ¢n háº¡ng nháº¥t theo tÃºng cÃ´ng dÃ¢n háº¡ng hai vÃ  ba. NgÆ°á»i gÃµ phÃ­m lÃ  Ã´ng chá»§, lÃ  trÃ¹m cuá»‘i. MÃ¬nh thÃ´ng qua cÃ´ng dÃ¢n Ä‘á»ƒ hiá»ƒu vÃ  tÆ°Æ¡ng tÃ¡c vá»›i trÃ¹m cuá»‘i. ÄÆ°á»£c quyá»n vÃ  Ä‘Æ°á»£c lá»±a chá»n tÆ°Æ¡ng tÃ¡c vá»›i cÃ´ng dÃ¢n háº¡ng nÃ o lÃ  má»™t lá»£i tháº¿, lá»›n trong cÃ¡ch tiáº¿p cáº­n rá»“i. Sau Ä‘Ã³ má»›i lÃ  cÃ¡c phÆ°Æ¡ng phÃ¡p (google vÃ´ sá»‘) Ä‘á»ƒ xá»­ lÃ½ bÃ i toÃ¡n.

Káº¾T

CÃ¡ch tiáº¿p cáº­n `subword` sáº½ lÃ  ná»n táº£ng, cÃ¡ch tiáº¿p cáº­n `keystrokes` sáº½ lÃ  bÆ°á»›c ngoáº·t trong nháº­p liá»‡u vÃ  tÆ°Æ¡ng tÃ¡c ngÆ°á»i mÃ¡y thÃ´ng qua ngÃ´n ngá»¯ tá»± nhiÃªn! VÃ  TOKEN, TOKENIZE, TOKENIZATION: WHAT IS TOKEN?, WHY TOKENIZATION?, TOKENIZE FOR WHAT? lÃ  bÆ°á»›c khá»Ÿi Ä‘áº§u vÃ´ cÃ¹ng quan trá»ng cho viá»‡c phÃ¡t triá»ƒn vá» sau. Bá»Ÿi Ä‘Ã³ lÃ  quÃ¡ trÃ¬nh Ä‘á»‹nh nghÄ©a vÃ  táº¡o ra cÃ¡c Ä‘Æ¡n vá»‹ thÃ´ng tin lÃ m ná»n táº£ng cho cÃ¡c bÆ°á»›c xá»­ lÃ½ tiáº¿p theo. 

CÃ¡c Ä‘Æ¡n vá»‹ cáº§n mang thÃ´ng tin cÃ³ Ã½ nghÄ©a, sá»‘ lÆ°á»£ng vá»«a Ä‘á»§ (quÃ¡ Ã­t thÃ¬ khÃ´ cá»©ng, quÃ¡ nhiá»u thÃ¬ loáº¡n), cÃ³ kháº£ nÄƒng chá»‹u lá»—i, cÃ³ kháº£ nÄƒng má»Ÿ rá»™ng Ä‘Æ°á»£c, cÃ³ kháº£ nÄƒng chuyá»ƒn hoÃ¡ Ä‘Æ°á»£c thÃ nh chuá»—i tÃ­n hiá»‡u Ä‘áº§u vÃ o gá»‘c (chuyá»ƒn hoÃ¡ hai chiá»u), khÃ´ng bá»‹ giá»›i háº¡n bá»Ÿi quy Æ°á»›c (cÃ¡ch gÃµ, cÃ¡ch mÃ£ hoÃ¡), bá»Ÿi ngÃ´n ngá»¯ ... sáº½ lÃ  tiÃªu chuáº©n Ä‘á»ƒ phÃ¡t triá»ƒn vÃ  xÃ¢y dá»±ng TOKENIZERS. NÃªn nhá»› word-tokenizer khÃ¡c, syllable-tokenizer khÃ¡c, subword-tokenizer khÃ¡c vÃ  keystroke-tokenizer lÃ  1 thá»© khÃ¡c ná»¯a ...

- - -

PHá»¤ Lá»¤C: DÃ¢y mÆ¡ rá»… mÃ¡ vÃ  Ä‘á»c thÃªm

Google vá» BPE dáº«n tá»›i [2], [2] dáº«n tá»›i [3], [3] dáº«n tá»›i [4] vÃ  [4] chá»‰ ra BPE Ä‘Æ°á»£c dÃ¹ng trong [5]: Google SentPiece.


[0] https://www.analyticsvidhya.com/blog/2020/05/what-is-tokenization-nlp/

[1] https://colah.github.io/posts/2014-07-NLP-RNNs-Representations/

[2] https://paperswithcode.com/method/bpe

[3] https://leimao.github.io/blog/Byte-Pair-Encoding

[4] https://en.wikipedia.org/wiki/Byte_pair_encoding

[5] https://github.com/google/sentencepiece

[6] https://jacky2wong.medium.com/understanding-sentencepiece-under-standing-sentence-piece-ac8da59f6b08

[7] https://gist.github.com/atinsood/6d185dfe025cbb5d55f158d4d17bc142