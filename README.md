# YOU SHOULD REWRITE

_Vi·∫øt l·∫°i modules quan tr·ªçng t·ª´ C sang Zig ƒë·ªÉ hi·ªÉu thu·∫≠t to√°n v√† nhu·∫ßn nhuy·ªÖn Zig_

* _BPE sub-word_ https://github.com/telexyz/tools/tree/master/vocab/fastBPE/fastBPE

BPE quan tr·ªçng trong vi·ªác chia nh·ªè OOV v√† √°nh x·∫° OOV v·ªÅ m·ªôt t·∫≠p tokens c√≥ s·ªë l∆∞·ª£ng ƒë·ªãnh tr∆∞·ªõc, nh·ªù ƒë√≥ ki·ªÉm so√°t t·ªët s·ªë l∆∞·ª£ng t·ª´ v·ª±ng, h·ª£p v·ªõi vi·ªác hu·∫•n luy·ªán m√¥ h√¨nh c√≥ t√†i nguy√™n h·∫°n ch·∫ø.

* _Word2Vec_ https://github.com/zhezhaoa/ngram2vec/blob/master/word2vec/word2vec.c

M·ªôt module quan tr·ªçng trong vi·ªác tr√¨nh b√†y l·∫°i token d∆∞·ªõi d·∫°ng vector trong kh√¥ng gian kho·∫£ng 300 chi·ªÅu, quan tr·ªçng trong vi·ªác t√¨m ki·∫øm token gi·ªëng nhau, d√πng ƒë·ªÉ train NN/LM, re-raking, re-scoring ...

https://aegis4048.github.io/optimize_computational_efficiency_of_skip-gram_with_negative_sampling


# STAGE-01 (90% done, xem `docs/.01-viet_token.md`)

# STAGE-02 (tri·ªÉn khai `docs/.02-syllabling.md`)

## TODOs

### Module 1/ `syllables2words`: g·ªôp √¢m ti·∫øt th√†nh t·ª´

1.1/ Naive Impl

* D√πng t·ª´ ƒëi·ªÉn li·ªát k√™ m·ªçi kh·∫£ nƒÉng t√°ch t·ª´

* Vi·∫øt beam-search decoder ƒë·ªÉ ch·ªçn +10 kh·∫£ nƒÉng t√°ch t·ª´ m√† √≠t √¢m ti·∫øt `m·ªì c√¥i` nh·∫•t

* Scoring d·ª±a tr√™n syllable n-grams, gi·ªØ l·∫°i 5-best

1.2/ Hu·∫•n luy·ªán ƒë∆∞·ª£c b·ªô t√°ch t·ª´. Tham kh·∫£o `docs/tach_tu_Nhat.md`

1.3/ Th·ª≠ √°p `src/syllabling/ripple_down_rules.zig` xem c√≥ g√¨ th√∫ v·ªã kh√¥ng?

[ >>> HERE I SHOULD BE, DOWN THE RABBIT HOLE <<< ]

* T·ª´ `dict/34k.xyz` v√† h√†m `parseXyzToGetSyllable()` chuy·ªÉn th√†nh 4-grams (t·ª´ <= 4 √¢m ti·∫øt) v√† ƒë·ªãnh danh b·∫±ng ph·∫ßn c√≤n l·∫°i c·ªßa `u16` (sau khi ƒë√£ tr·ª´ ƒëi syllable ids v√† OOV BPE).  Kh·ªüi t·∫°o t·ª´ ƒëi·ªÉn gi·ªëng nh∆∞ hash_count c·ªßa n-gram nh∆∞ng ƒë∆°n gi·∫£n h∆°n v√¨ ch·ªâ c·∫ßn ch·ª©a 32k items (2^15). C√≥ l·∫Ω ch·ªâ c·∫ßn `u24 hash` ƒë·ªÉ map v√†o `2^16 slots`, m·ªói slot ch·ª©a `u16` word id.

### Module 0/ n-gram n√¢ng cao

* Cho 1 phrase (nhi·ªÅu syllables), t√≠nh x√°c xu·∫•t d·ª±a tr√™n n-gram count

* L√†m m∆∞·ª£t n-gram `data/2{n}-grams` b·∫±ng absoluted discount v√† stupid backoff

[ >>> DONE <<< ]

### Module 1/ `syllables2words`: g·ªôp √¢m ti·∫øt th√†nh t·ª´

### Module 0/ n-gram n√¢ng cao

* T√¨m m·ªôt c√°ch ƒë∆°n gi·∫£n ƒë·ªÉ l√†m m∆∞·ª£t n-grams (xem `docs/n-gram_smoothing.md`)

* T√¨m c√°ch t·ªëi ∆∞u nh·∫•t, √≠t b·ªô nh·ªõ nh·∫•t ƒë·ªÉ load n-gram (xem `counting/language_model.zig`)

* T·ª´ `data/{21,..28}-n_grams.txt` t√¨m c√°ch load v√† hashing v·ªÅ `u64` xem c√≥ b·ªã va ch·∫°m kh√¥ng?

* T√¨m c√°ch ƒë·∫øm, l∆∞u n-gram (bao g·ªìm t·ª´ ƒëi·ªÉn) hi·ªáu qu·∫£ nh·∫•t `docs/n-gram_lookup_IFM_trie.md`

* Tham kh·∫£o ki·∫øn tr√∫c t√≠ch h·ª£p Jumanpp

- - -

# STAGE-01: B√°o c√°o th√†nh t√≠ch

[ GOAL ] PH√ÅT HI·ªÜN, TR√åNH B√ÄY L·∫†I V√Ä INDEX TOKENS SAO CHO C√ì TH·ªÇ XEM+X√âT CORPUS TH·∫¨T NHANH, LO·∫†I B·ªé D·ªÆ LI·ªÜU TR√ôNG L·∫∂P, PH√ÅT HI·ªÜN C√ÅC TR∆Ø·ªúNG H·ª¢P B·∫§T THU√íNG, T·ª∞ ƒê·ªòNG S·ª¨A L·ªñI, B·ªé ƒêI NH·ªÆNG ƒêO·∫†N TEXT K√âM CH·∫§T L∆Ø·ª¢NG

D∆∞·ªõi g√≥c nh√¨n qu·∫£n tr·ªã d·ªØ li·ªáu c√≤n r·∫•t nhi·ªÅu vi·ªác th√∫ v·ªã ƒë·ªÉ l√†m nh∆∞ l√† ph√°t hi·ªán v√† lo·∫°i b·ªè tr√πng l·∫∑p, indexing ƒë·ªÉ t√¨m ki·∫øm v√† so s√°nh nhanh c√°c c√¢u trong corpus, ph√°t hi·ªán s·ª± thi·∫øu, th·ª´a, sai / kh√¥ng ho√†n ch·ªânh ... c·ªßa d·ªØ li·ªáu.

N·∫øu coi corpus l√† m·ªôt file text l·ªõn, m·ªói c√¢u ƒë∆∞·ª£c ch·ª©a tr√™n m·ªôt d√≤ng, m·ªói d√≤ng kho·∫£ng 12.5 tokens, th√¨ 10 tri·ªáu d√≤ng chi·∫øm kho·∫£ng 600MB. M·ªói file text l·ªõn c√≥ file index (.idx) ri√™ng ƒëi k√®m, t∆∞∆°ng t·ª± nh∆∞ c√≥ file th√¥ng tin tr√≠ch xu·∫•t nh∆∞ ƒë·ªãnh danh / m√£ ho√° (.cdx) ri√™ng ƒëi k√®m.

D√πng `u32` ƒë·ªÉ ƒë·ªãnh danh th√¨ s·∫Ω ch·ª©a ƒë∆∞·ª£c g·∫ßn 4.3 t·ªâ ƒë·∫ßu m·ª•c, t∆∞∆°ng ƒë∆∞∆°ng v·ªõi 1 file text copus 2.1Tb. D·ªØ li·ªáu https://pile.eleuther.ai cho m√¥ h√¨nh ng√¥n ng·ªØ si√™u kh·ªßng m·ªõi ch·ªâ ·ªü m·ª©c 0.8Tb (800GB).


## Th√†nh t·ª±u ch√≠nh

* D√πng √¢m v·ªã h·ªçc ƒë·ªÉ ph√¢n t√≠ch v√† ƒë·ªãnh danh nhanh m·ªçi √¢m ti·∫øt TV vi·∫øt th∆∞·ªùng th√†nh `15-bits` m√† kh√¥ng c·∫ßn d√πng d·ªØ li·ªáu ƒë·ªëi chi·∫øu (lookup-table, trie, ...) ƒë·ªÉ chuy·ªÉn t·ª´ d·∫°ng text th√†nh ƒë·ªãnh danh c≈©ng nh∆∞ t·ª´ ƒë·ªãnh danh 16-bits kh√¥i ph·ª•c l·∫°i d·∫°ng text c·ªßa √¢m ti·∫øt. (xem `src/syllable_data_struct.zig`). 

S·ªë l∆∞·ª£ng √¢m ti·∫øt ti·∫øng Vi·ªát vi·∫øt th∆∞·ªùng l·ªçc t·ª´ corpus r∆°i v√†o kho·∫£ng 12k. [HieuThi](http://www.hieuthi.com/blog/2017/03/21/all-vietnamese-syllables.html) ch·ªâ ra TV c√≥ kho·∫£ng 18k √¢m ti·∫øt, ch·ª©ng t·ªè kho·∫£ng 6k (33%) √¢m ti·∫øt c√≥ th·ªÉ ƒë√∫ng v·ªÅ m·∫∑t gh√©p √¢m nh∆∞ng kh√¥ng ƒë∆∞·ª£c ho·∫∑c r·∫•t √≠t khi ƒë∆∞·ª£c s·ª≠ d·ª•ng. 18k √¢m ti·∫øt th√¨ ph·∫£i d√πng `15-bits` ƒë·ªÉ ƒë·ªãnh danh, kh√¥ng th·ªÉ √≠t h∆°n. N·∫øu d√πng `16-bits` (2-bytes) ƒë·ªÉ ƒë·ªãnh danh tokens th√¨ d√πng `15-bits cao` ƒë·ªÉ l∆∞u t·ª´ ƒëi·ªÉn TV `32_768` v√† ph·∫ßn c√≤n l·∫°i c·ªßa `15-bits th·∫•p` ƒë·ªÉ ch·ª©a OOV `6_516`. N·∫øu c·∫ßn nhi·ªÅu slots h∆°n cho OOV th√¨ g·∫°n xem syllable_ids n√†o invalid (ko ƒë√∫ng lu·∫≠t ng·ªØ √¢m) s·∫Ω ƒëc th√™m `8_370`.

* Th·ªëng k√™ v√† li·ªát k√™ token types theo freqs v√† length, ph√¢n chia th√†nh token trong b·∫£ng ch·ªØ c√°i c√≥ d·∫•u + thanh `alphamark`, token trong b·∫£ng ch·ªØ c√°i kh√¥ng d·∫•u thanh `alpha0m0t`, token kh√¥ng thu·ªôc b·∫£ng ch·ªØ c√°i `nonalpha`, nh·ªù ƒë√≥ ph√°t hi·ªán nhanh token b·∫•t th∆∞·ªùng, token l·ªói ... (xem https://github.com/telexyz/results#readme)

* Th·ª≠ nghi·ªám v·ªõi g·∫ßn 1Gb text tr·ªôn t·ª´ Facebook comments, news titles, viet opensub, wikipedia, s√°ch, truy·ªán .. Trong v√≤ng 45 gi√¢y ph√¢n t√°ch ƒë∆∞·ª£c: 
```r
 73% tokens √¢m ti·∫øt ti·∫øng Vi·ªát  148_280_481  "c·ªßa v√† c√≥ kh√¥ng l√† ƒë∆∞·ª£c cho c√°c"
  6% tokens thu·ªôc b·∫£ng ch·ªØ c√°i   11_953_258  "ƒë ƒëc Nƒê ƒêH TP USD inbox shop"
 21% tokens ngo√†i b·∫£ng ch·ªØ c√°i   43_576_527  ". , - : ? ; '' " 1 ! 2 / ... 2020 ü§£ 19000019"
(18% ngo√†i b·∫£ng ch·ªØ c√°i 1 k√Ω t·ª±  37_108_988) ". , - : ? ; ' 1 2 /"
- - - - - - - - - - - - - - - - - - - - - -
100% t·ªïng tokens                203_810_266
```
=> TRUNG B√åNH M·ªòT GI√ÇY PH√ÇN T√ÅCH V√Ä PH√ÇN LO·∫†I 5 TRI·ªÜU TOKENS, ƒê·ªäNH DANH 3.65 TRI·ªÜU √ÇM TI·∫æT TV
Tr√™n m√°y macbook 2015, 8Gb ram, 1.1Ghz Duo-core Intel M

_ƒê·ªëi chi·∫øu_

1/ https://github.com/phuonglh/vn.vitk | https://github.com/phuonglh/jlp can tokenize a text of two million Vietnamese syllables in 20 seconds on a cluster of three computers (24 cores, 24 GB RAM), giving an accuracy of about 97%

2/ https://github.com/vncorenlp/VnCoreNLP
![](docs/files/vn_word_segmenters_speed.png)[width:600]


- - -

[ BY PRODUCT 1 ] Th·ªëng k√™ t·ª´ v·ª±ng v√† n-gram c∆° b·∫£n, s·ª≠a l·ªói ch√≠nh t·∫£ ƒë∆°n gi·∫£n d·ª±a tr√™n ph√¢n t√≠ch √¢m v·ªã h·ªçc ...

[ BY PRODUCT 2 ] C·∫£i ti·∫øn b·ªô g√µ Telex, d√πng `az,ez,oz` thay `aa,ee,oo` ƒë·ªÉ th·ªëng nh·∫•t v·ªõi c√°ch b·ªè d·∫•u nh∆∞ `aw,ow,uw`; ch·ªâ b·ªè d·∫•u v√† thanh cu·ªëi √¢m ti·∫øt `nuoc|ws`

__C√¢u h·ªèi ƒë·∫∑t ra:__

Y·∫øu ƒëi·ªÉm c√≥ th·ªÉ coi l√† l·ªõn nh·∫•t c·ªßa Telex l√† vi·∫øt song ng·ªØ r·∫•t ch·∫≠m,
v√¨ hay b·ªã hi·ªÉu l·∫ßm th√†nh d·∫•u m≈©. Vi·ªác chuy·ªÉn b√†n ph√≠m th√¨ c≈©ng r·∫•t m·∫•t th·ªùi gian !!!
L√†m th·∫ø n√†o ƒë·ªÉ gi·∫£m thi·ªÉu s·ª± nh·∫ßm l·∫´n khi g√µ ti·∫øng Anh l·∫´n l·ªôn v·ªõi ti·∫øng Vi·ªát ???
Vi·∫øt ho√†n to√†n kh√¥ng d·∫•u v√† ƒë·ªÉ m√°y t·ª± b·ªè d·∫•u v·ªõi s·ª± tr·ª£ gi√∫p t·ª´ ng∆∞·ªùi d√πng ???