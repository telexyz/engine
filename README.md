### stage-1: 99% done `docs/*1-viet_token.md`

### stage-2: 60% done `docs/*2-syllabling.md`

### stage-3: 10% done `https://github.com/telexyz/fingers`

### stage-4: 00% done `docs/*_4-index_control.md`

stage-1/ c√≤n BPE v√† t·∫°o danh s√°ch c√°c t·ª´ ti·∫øng Vi·ªát hi·∫øm/vay m∆∞·ª£n kh√¥ng ph·∫£i syllables. Hi·ªán t·∫°i ch·ªâ t·∫≠p trung v√†o syllables n√™n ph·∫ßn n√†y c√≥ th·ªÉ g√°c qua m·ªôt b√™n v√† coi nh∆∞ xong!

stage-2/ ho√†n thi·ªán th·∫≠t ƒë∆°n gi·∫£n, ch·ªâ c·∫ßn dict_matching ra m·ªçi kh·∫£ nƒÉng c√≥ th·ªÉ g·ªôp √¢m ti·∫øt th√†nh t·ª´ l√† xong. M√¥ h√¨nh ng√¥n ng·ªØ s·ª≠ d·ª•ng n-gram tinh g·ªçn ƒë·ªÉ c√≥ th·ªÉ nh√∫ng v√†o wasm.

stage-3/ tri·ªÉn khai nhanh ƒë·ªÉ li√™n t·ª•c th·ª≠ nghi·ªám v√† c·∫£i ti·∫øn

stage-4/ th√∫ v·ªã, ƒë√£ ch·ªët ƒë∆∞·ª£c ph∆∞∆°ng √°n l√†m inverted indexing !!! Sau khi index c√≥ th·ªÉ qu·∫£n t·ª´ng token m·ªôt t·ªõi ƒë·ªô s√¢u ·ªü m·ª©c vƒÉn b·∫£n, ƒëo·∫°n, c√¢u, t·ª´, d∆∞·ªõi t·ª´ ... t√πy th√≠ch. Ch·ªâ c·∫ßn bi·∫øt token ƒëang ·ªü ƒë√¢u ·ªü m·ª©c ph√¢n ƒëo·∫°n, sau ƒë√≥ d√πng b·ªô parser m·∫°nh ƒë√£ x√¢y d·ª±ng ·ªü stage-1 ƒë·ªÉ b√≥c t√°ch ti·∫øp.

- - -

# N·ª≠a nƒÉm nh√¨n l·∫°i (01/2022-06/2022)

Th·ªùi gian n√†y t√¨m hi·ªÉu k·ªπ h∆°n v·ªÅ m√¥ h√¨nh ng√¥n ng·ªØ d√πng deep learning, ki·∫øn tr√∫c transformer, v√† ƒë·∫∑c bi·ªát c√°c ki·∫øn tr√∫c tinh g·ªçn ƒë·ªÉ gi·∫£m th·ªùi gian hu·∫•n luy·ªán v√† k√≠ch th∆∞·ªõc m√¥ h√¨nh m√† ko l√†m gi·∫£m qu√° nhi·ªÅu ƒë·ªô ch√≠nh x√°c.

C√°c m√¥ h√¨nh ng√¥n ng·ªØ l·ªõn, li√™n quan t·ªõi deep learning th∆∞·ªùng ƒë√≤i h·ªèi l∆∞·ª£ng t√≠nh to√°n l·ªõn (ch·∫°y v√†i ng√†y th·∫≠m ch√≠ v√†i tu·∫ßn tr√™n 1 m√°y t√≠nh c·ª±c m·∫°nh, 4 GPUs ch·∫≥ng h·∫°n), n√™n c√≥ s·ª± h·∫°n ch·∫ø cho ng∆∞·ªùi m·ªõi b·∫Øt ƒë·∫ßu. H·ªç th∆∞·ªùng th·∫•y "ng·ª£p" v·ªÅ s·ªë l∆∞·ª£ng d·ªØ li·ªáu c·∫ßn thu th·∫≠p, v·ªÅ c·∫•u h√¨nh m√°y t√≠nh t·ªëi thi·ªÉu v√† "n·∫£n" khi ph·∫£i ch·ªù ƒë·ª£i l√¢u m·ªõi c√≥ k·∫øt qu·∫£.

Ph·∫ßn n√†y n·∫øu s·ª± d·ª•ng l·∫°i LLM (large language model) ƒë∆∞·ª£c hu·∫•n luy·ªán b·ªüi b√™n th√∫ 3 ho·∫∑c k·∫øt h·ª£p v·ªõi ƒë·ªëi t√°c ƒë·ªÉ th·ª≠ nghi·ªám v√† tri·ªÉn khai th√¨ h·ª£p l√Ω h∆°n. Ho·∫∑c c·∫ßn 1 nh√¢n s·ª± chuy√™n t√¢m khai th√°c m·∫£ng n√†y.

C√≥ nhi·ªÅu ƒëi·ªÅu th√∫ v·ªã c·∫ßn ƒë∆∞·ª£c ki·ªÉm ch·ª©ng nh∆∞ c√°c c√°ch tokenize ti·∫øng Vi·ªát s·∫Ω ·∫£nh h∆∞·ªüng t·ªõi ƒë·ªô ch√≠nh x√°c c·ªßa m√¥ h√¨nh nhi·ªÅu hay √≠t? Vi·ªác l·ªçc d·ªØ li·ªáu hay l·ª±a ch·ªçn d·ªØ li·ªáu c·∫©n th·∫≠n c≈©ng v·∫≠y. R·ªìi sau ƒë√≥ t·ªõi vi·ªác ch·ªçn m·ªô m√¥ h√¨nh ph√π h·ª£p v·ªõi b·ªô d·ªØ li·ªáu ƒëang c√≥. C√°c c√°ch ti·∫øp c·∫≠n kh√°c ngo√†i deep learning v√≠ d·ª• nh∆∞ online learning / active learning cho b·ªô g√µ v√¨ khi t∆∞∆°ng t√°c v·ªõi ng∆∞·ªùi d√πng ta li√™n t·ª•c nh·∫≠n ƒë∆∞·ª£c nh·ªØng ph·∫£n h·ªìi v√† d·ªØ li·ªáu m·ªõi nh·∫•t, c·∫≠p nh·∫≠t nh·∫•t. C·∫ßn m·ªôt c√°ch ti·∫øp c·∫≠n ƒë·ªÉ khai th√°c ƒë∆∞·ª£c l∆∞·ª£ng d·ªØ li·ªáu t∆∞∆°i m·ªõi n√†y, v√† m√¥ h√¨nh c≈©ng c·∫ßn ph·∫£n h·ªìi nhanh v√¨ b·ªô g√µ c·∫ßn t∆∞∆°ng t√°c v·ªõi ƒë·ªô tr·ªÖ th·∫•p nh·∫•t c√≥ th·ªÉ ƒë·ªÉ t·∫°o ra c·∫£m gi√°c m∆∞·ª£t m√† khi s·ª≠ d·ª•ng.

# N·ª≠a nƒÉm nh√¨n l·∫°i (06/2021-12/2021)

[ M·ª§C TI√äU BAN ƒê·∫¶U ] PH√ÅT HI·ªÜN, TR√åNH B√ÄY L·∫†I V√Ä INDEX TOKENS SAO CHO C√ì TH·ªÇ XEM+X√âT CORPUS TH·∫¨T NHANH, LO·∫†I B·ªé D·ªÆ LI·ªÜU TR√ôNG L·∫∂P, PH√ÅT HI·ªÜN C√ÅC TR∆Ø·ªúNG H·ª¢P B·∫§T THU√íNG, T·ª∞ ƒê·ªòNG S·ª¨A L·ªñI, B·ªé ƒêI NH·ªÆNG ƒêO·∫†N TEXT K√âM CH·∫§T L∆Ø·ª¢NG

D∆∞·ªõi g√≥c nh√¨n qu·∫£n tr·ªã d·ªØ li·ªáu c√≤n r·∫•t nhi·ªÅu vi·ªác th√∫ v·ªã ƒë·ªÉ l√†m nh∆∞ l√† ph√°t hi·ªán v√† lo·∫°i b·ªè tr√πng l·∫∑p, indexing ƒë·ªÉ t√¨m ki·∫øm v√† so s√°nh nhanh c√°c c√¢u trong corpus, ph√°t hi·ªán s·ª± thi·∫øu, th·ª´a, sai / kh√¥ng ho√†n ch·ªânh ... c·ªßa d·ªØ li·ªáu.

N·∫øu coi corpus l√† m·ªôt file text l·ªõn, m·ªói c√¢u ƒë∆∞·ª£c ch·ª©a tr√™n m·ªôt d√≤ng, m·ªói d√≤ng kho·∫£ng 12.5 tokens, th√¨ 10 tri·ªáu d√≤ng chi·∫øm kho·∫£ng 600MB. M·ªói file text l·ªõn c√≥ file index (.idx) ri√™ng ƒëi k√®m, t∆∞∆°ng t·ª± nh∆∞ c√≥ file th√¥ng tin tr√≠ch xu·∫•t nh∆∞ ƒë·ªãnh danh / m√£ ho√° (.cdx) ri√™ng ƒëi k√®m.

D√πng `u32` ƒë·ªÉ ƒë·ªãnh danh th√¨ s·∫Ω ch·ª©a ƒë∆∞·ª£c g·∫ßn 4.3 t·ªâ ƒë·∫ßu m·ª•c, t∆∞∆°ng ƒë∆∞∆°ng v·ªõi 1 file text copus 2.1Tb. D·ªØ li·ªáu https://pile.eleuther.ai cho m√¥ h√¨nh ng√¥n ng·ªØ si√™u kh·ªßng m·ªõi ch·ªâ ·ªü m·ª©c 0.8Tb (800GB).


## Th√†nh t·ª±u ch√≠nh

* D√πng √¢m v·ªã h·ªçc ƒë·ªÉ ph√¢n t√≠ch v√† ƒë·ªãnh danh nhanh m·ªçi √¢m ti·∫øt TV vi·∫øt th∆∞·ªùng th√†nh `15-bits` m√† kh√¥ng c·∫ßn d√πng d·ªØ li·ªáu ƒë·ªëi chi·∫øu (lookup-table, trie, ...) ƒë·ªÉ chuy·ªÉn t·ª´ d·∫°ng text th√†nh ƒë·ªãnh danh c≈©ng nh∆∞ t·ª´ ƒë·ªãnh danh 16-bits kh√¥i ph·ª•c l·∫°i d·∫°ng text c·ªßa √¢m ti·∫øt. (xem `src/syllable_data_struct.zig`). 

S·ªë l∆∞·ª£ng √¢m ti·∫øt ti·∫øng Vi·ªát vi·∫øt th∆∞·ªùng l·ªçc t·ª´ corpus r∆°i v√†o kho·∫£ng 12k. [HieuThi](http://www.hieuthi.com/blog/2017/03/21/all-vietnamese-syllables.html) ch·ªâ ra TV c√≥ kho·∫£ng 18k √¢m ti·∫øt, ch·ª©ng t·ªè kho·∫£ng 6k (33%) √¢m ti·∫øt c√≥ th·ªÉ ƒë√∫ng v·ªÅ m·∫∑t gh√©p √¢m nh∆∞ng kh√¥ng ƒë∆∞·ª£c ho·∫∑c r·∫•t √≠t khi ƒë∆∞·ª£c s·ª≠ d·ª•ng. 18k √¢m ti·∫øt th√¨ ph·∫£i d√πng `15-bits` ƒë·ªÉ ƒë·ªãnh danh, kh√¥ng th·ªÉ √≠t h∆°n. N·∫øu d√πng `16-bits` (2-bytes) ƒë·ªÉ ƒë·ªãnh danh tokens th√¨ d√πng `15-bits cao` ƒë·ªÉ l∆∞u t·ª´ ƒëi·ªÉn TV `32_768` v√† ph·∫ßn c√≤n l·∫°i c·ªßa `15-bits th·∫•p` ƒë·ªÉ ch·ª©a OOV `6_516`. N·∫øu c·∫ßn nhi·ªÅu slots h∆°n cho OOV th√¨ g·∫°n xem syllable_ids n√†o invalid (ko ƒë√∫ng lu·∫≠t ng·ªØ √¢m) s·∫Ω ƒëc th√™m `8_370`.

* Th·ªëng k√™ v√† li·ªát k√™ token types theo freqs v√† length, ph√¢n chia th√†nh token trong b·∫£ng ch·ªØ c√°i c√≥ d·∫•u + thanh `alphamark`, token trong b·∫£ng ch·ªØ c√°i kh√¥ng d·∫•u thanh `alpha0m0t`, token kh√¥ng thu·ªôc b·∫£ng ch·ªØ c√°i `nonalpha`, nh·ªù ƒë√≥ ph√°t hi·ªán nhanh token b·∫•t th∆∞·ªùng, token l·ªói ... (xem https://github.com/telexyz/results#readme)

* Th·ª≠ nghi·ªám v·ªõi g·∫ßn 1Gb text tr·ªôn t·ª´ Facebook comments, news titles, viet opensub, wikipedia, s√°ch, truy·ªán .. Trong v√≤ng 45 gi√¢y ph√¢n t√°ch ƒë∆∞·ª£c: 
```r
 73% tokens √¢m ti·∫øt ti·∫øng Vi·ªát  148_280_481  "c·ªßa v√† c√≥ kh√¥ng l√† ƒë∆∞·ª£c cho c√°c"
  6% tokens thu·ªôc b·∫£ng ch·ªØ c√°i   11_953_258  "ƒë ƒëc Nƒê ƒêH TP USD inbox shop"
 21% tokens ngo√†i b·∫£ng ch·ªØ c√°i   43_576_527  ". , - : ? ; '' " 1 ! 2 / ... 2020 ü§£ 19000019"
(18% ngo√†i b·∫£ng ch·ªØ c√°i 1 k√Ω t·ª±  37_108_988) ". , - : ? ; ' 1 2 /"
- - - - - - - - - - - - - - - - - - - - - -
100% t·ªïng tokens                203_810_266
```
=> TRUNG B√åNH M·ªòT GI√ÇY PH√ÇN T√ÅCH V√Ä PH√ÇN LO·∫†I 5 TRI·ªÜU TOKENS, ƒê·ªäNH DANH 3.65 TRI·ªÜU √ÇM TI·∫æT TV
Tr√™n m√°y macbook 2015, 8Gb ram, 1.1Ghz Duo-core Intel M

_ƒê·ªëi chi·∫øu_

1/ https://github.com/phuonglh/vn.vitk | https://github.com/phuonglh/jlp can tokenize a text of two million Vietnamese syllables in 20 seconds on a cluster of three computers (24 cores, 24 GB RAM), giving an accuracy of about 97%

2/ https://github.com/vncorenlp/VnCoreNLP
![](docs/files/vn_word_segmenters_speed.png){width=720 height=320}


- - -

[ BY PRODUCT 1 ] Th·ªëng k√™ t·ª´ v·ª±ng v√† n-gram c∆° b·∫£n, s·ª≠a l·ªói ch√≠nh t·∫£ ƒë∆°n gi·∫£n d·ª±a tr√™n ph√¢n t√≠ch √¢m v·ªã h·ªçc ...

[ BY PRODUCT 2 ] C·∫£i ti·∫øn b·ªô g√µ Telex, d√πng `az,ez,oz` thay `aa,ee,oo` ƒë·ªÉ th·ªëng nh·∫•t v·ªõi c√°ch b·ªè d·∫•u nh∆∞ `aw,ow,uw`; ch·ªâ b·ªè d·∫•u v√† thanh cu·ªëi √¢m ti·∫øt `nuoc|ws`

__C√¢u h·ªèi ƒë·∫∑t ra:__

Y·∫øu ƒëi·ªÉm c√≥ th·ªÉ coi l√† l·ªõn nh·∫•t c·ªßa Telex l√† vi·∫øt song ng·ªØ r·∫•t ch·∫≠m,
v√¨ hay b·ªã hi·ªÉu l·∫ßm th√†nh d·∫•u m≈©. Vi·ªác chuy·ªÉn b√†n ph√≠m th√¨ c≈©ng r·∫•t m·∫•t th·ªùi gian !!!
L√†m th·∫ø n√†o ƒë·ªÉ gi·∫£m thi·ªÉu s·ª± nh·∫ßm l·∫´n khi g√µ ti·∫øng Anh l·∫´n l·ªôn v·ªõi ti·∫øng Vi·ªát ???
Vi·∫øt ho√†n to√†n kh√¥ng d·∫•u v√† ƒë·ªÉ m√°y t·ª± b·ªè d·∫•u v·ªõi s·ª± tr·ª£ gi√∫p t·ª´ ng∆∞·ªùi d√πng ???
